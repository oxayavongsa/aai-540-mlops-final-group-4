{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d097e62-2546-47c0-9b15-31d2587e39f5",
   "metadata": {},
   "source": [
    "## Model Monitoring Setup\n",
    "Data Capture, Baseline Generation, Deployment Preparation for Model Quality Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ebb3604-fada-4a50-8bfb-22161d040510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.sklearn.model import SKLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8337e5d1-ffe5-4c91-b402-e59817e5622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker session initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize session and role\n",
    "session = Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Define bucket and paths\n",
    "bucket = 'sagemaker-us-east-1-531690656306'\n",
    "prefix = 'cardio_data'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(\"SageMaker session initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7f1151b2-b6b0-4bf2-9cc5-804ca8971ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  gender  height_ft  weight_lbs  systolic_bp  diastolic_bp  cholesterol  \\\n",
      "0   50       2       5.51      136.69          110            80            1   \n",
      "1   55       1       5.12      187.39          140            90            3   \n",
      "2   51       1       5.41      141.10          130            70            3   \n",
      "3   48       2       5.54      180.78          150           100            1   \n",
      "4   47       1       5.12      123.46          100            60            1   \n",
      "\n",
      "   gluc  smoke  alco  ...  cholesterol_label  pulse_pressure  chol_bmi_ratio  \\\n",
      "0     1      0     0  ...             Normal              30            4.55   \n",
      "1     1      0     0  ...  Well Above Normal              50            8.60   \n",
      "2     1      0     0  ...  Well Above Normal              60           12.74   \n",
      "3     1      0     0  ...             Normal              50            3.48   \n",
      "4     1      0     0  ...             Normal              40            4.35   \n",
      "\n",
      "  height_in age_years  is_hypertensive  bp_category  bmi_category  \\\n",
      "0     66.12        50                0       stage1        normal   \n",
      "1     61.44        55                1       stage2         obese   \n",
      "2     64.92        51                0       stage1        normal   \n",
      "3     66.48        48                1       stage2    overweight   \n",
      "4     61.44        47                0       normal        normal   \n",
      "\n",
      "   age_gluc_interaction  lifestyle_score  \n",
      "0                    50               -1  \n",
      "1                    55               -1  \n",
      "2                    51                0  \n",
      "3                    48               -1  \n",
      "4                    47                0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cleaned dataset uploaded to S3'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your existing engineered dataset\n",
    "df = pd.read_csv('cardio_engineered.csv')\n",
    "\n",
    "# Check data\n",
    "print(df.head())\n",
    "\n",
    "# Save cleaned CSV\n",
    "clean_file = 'cardio_engineered_clean.csv'\n",
    "df.to_csv(clean_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Upload cleaned CSV to S3\n",
    "s3_client.upload_file(clean_file, bucket, f'{prefix}/cardio_engineered_clean.csv')\n",
    "\"Cleaned dataset uploaded to S3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a98909a8-e882-474f-baf3-a7011645f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and inference script\n",
    "model_artifact = f's3://{bucket}/model/logistic_model.tar.gz'\n",
    "entry_point = 'inference.py'\n",
    "endpoint_name = 'cardio-logistic-monitor-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "86282eab-2f3a-4d26-8bba-f2ff0c9bfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    entry_point=entry_point,\n",
    "    framework_version='0.23-1',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Enable full data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f's3://{bucket}/data-capture',\n",
    "    capture_options=['Request', 'Response']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d447eb66-fcaf-4be7-840e-3bcaebc6f0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'cardio-logistic-monitor-endpoint' already exists. Skipping deployment.\n"
     ]
    }
   ],
   "source": [
    "# Deploy only if endpoint doesn't exist\n",
    "def deploy_if_not_exists(model, endpoint_name, instance_type, data_capture_config):\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Endpoint '{endpoint_name}' already exists. Skipping deployment.\")\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if 'Could not find endpoint' in str(e):\n",
    "            model.deploy(\n",
    "                initial_instance_count=1,\n",
    "                instance_type=instance_type,\n",
    "                endpoint_name=endpoint_name,\n",
    "                data_capture_config=data_capture_config\n",
    "            )\n",
    "            print(f\"Deployed endpoint '{endpoint_name}' successfully.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Call deployment function\n",
    "deploy_if_not_exists(\n",
    "    model=sklearn_model,\n",
    "    endpoint_name=endpoint_name,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6983fe45-b4c2-4220-ab74-65cd97b0869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2025-06-11-02-13-39-244\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      ".............\u001b[34m2025-06-11 02:15:41.869534: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:41.869565: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:43.447189: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:43.447219: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:43.447242: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-216-44.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:43.447497: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:531690656306:processing-job/baseline-suggestion-job-2025-06-11-02-13-39-244', 'ProcessingJobName': 'baseline-suggestion-job-2025-06-11-02-13-39-244', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::531690656306:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,005 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,067 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,068 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,068 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,077 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,078 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,078 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,563 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.216.44\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoo\u001b[0m\n",
      "\u001b[34mp-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,570 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:45,574 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-6528f71b-b114-4fcb-b89c-aa5994bd064d\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,127 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,138 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,139 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,141 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,146 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,146 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,146 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,146 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,178 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,190 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,190 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,193 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,197 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jun 11 02:15:46\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,198 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,198 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,199 INFO util.GSet: 2.0% max memory 3.1 GB = 63.1 MB\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,199 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,235 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,238 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,238 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,238 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,238 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,239 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,264 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,264 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,265 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,265 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,279 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,279 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,279 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,279 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,284 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,287 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,287 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,287 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,287 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,293 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,293 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,293 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,296 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,296 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,298 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,298 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,298 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 968.9 KB\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,298 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,319 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1591639676-10.0.216.44-1749608146314\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,332 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,339 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,414 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,426 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,430 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.216.44\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:46,443 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:48,502 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:48,502 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:50,561 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:50,562 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:52,634 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:52,635 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:54,718 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:54,719 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:56,827 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:15:56,828 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:06,836 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:08,607 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,030 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,076 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,086 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,606 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,629 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,629 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,630 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,630 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,650 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,663 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,665 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,711 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,711 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,712 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,712 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:09,712 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,074 INFO util.Utils: Successfully started service 'sparkDriver' on port 40289.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,100 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,133 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,152 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,152 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,195 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,225 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-f0d108fc-3343-4cf4-bcc0-59b056a31b83\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,246 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,296 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,342 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.216.44:40289/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1749608169603\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:10,858 INFO client.RMProxy: Connecting to ResourceManager at /10.0.216.44:8032\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,531 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,532 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,538 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,538 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,539 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,539 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,545 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:11,621 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:13,917 INFO yarn.Client: Uploading resource file:/tmp/spark-881e7685-a5cc-49eb-9ab1-67e8f6e0a9e7/__spark_libs__998954140899195531.zip -> hdfs://10.0.216.44/user/root/.sparkStaging/application_1749608152089_0001/__spark_libs__998954140899195531.zip\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,094 INFO yarn.Client: Uploading resource file:/tmp/spark-881e7685-a5cc-49eb-9ab1-67e8f6e0a9e7/__spark_conf__4230663474294709474.zip -> hdfs://10.0.216.44/user/root/.sparkStaging/application_1749608152089_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,537 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,537 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,537 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,537 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,538 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,565 INFO yarn.Client: Submitting application application_1749608152089_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:15,758 INFO impl.YarnClientImpl: Submitted application application_1749608152089_0001\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:16,762 INFO yarn.Client: Application report for application_1749608152089_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:16,765 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: AM container is launched, waiting for AM container to Register with RM\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1749608175661\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1749608152089_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:17,770 INFO yarn.Client: Application report for application_1749608152089_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:18,773 INFO yarn.Client: Application report for application_1749608152089_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:19,777 INFO yarn.Client: Application report for application_1749608152089_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,781 INFO yarn.Client: Application report for application_1749608152089_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,781 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.216.44\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1749608175661\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1749608152089_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,783 INFO cluster.YarnClientSchedulerBackend: Application application_1749608152089_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,792 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 39025.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,792 INFO netty.NettyBlockTransferService: Server created on 10.0.216.44:39025\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,794 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,802 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.216.44, 39025, None)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,806 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.216.44:39025 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.216.44, 39025, None)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,816 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.216.44, 39025, None)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,818 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.216.44, 39025, None)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,839 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1749608152089_0001), /proxy/application_1749608152089_0001\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:20,938 INFO util.log: Logging initialized @13841ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:22,390 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:26,740 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.216.44:56438) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:26,948 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:42843 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 42843, None)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:40,793 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:40,984 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:41,040 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:41,046 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,003 INFO datasources.InMemoryFileIndex: It took 43 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,189 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,509 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,512 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.216.44:39025 (size: 39.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,517 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,884 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,887 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,893 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,949 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,968 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,968 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,969 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,970 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:42,981 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,028 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,031 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,032 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.216.44:39025 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,033 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,050 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,051 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,092 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4636 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:43,330 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:42843 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,148 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:42843 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,503 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1425 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,505 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,511 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.500 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,515 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,515 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,517 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.568198 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,684 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.216.44:39025 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:44,690 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:42843 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,653 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,655 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,658 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: string, gender: string, height_ft: string, weight_lbs: string, systolic_bp: string ... 22 more fields>\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,842 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,857 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,858 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.216.44:39025 (size: 39.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,859 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,875 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,914 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,915 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,915 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,916 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,919 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,926 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,980 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,983 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,983 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.216.44:39025 (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,984 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,985 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,985 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:46,988 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:47,028 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:42843 (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:47,755 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:42843 (size: 39.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,003 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:42843 (size: 3.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,115 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2129 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,115 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,116 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.187 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,116 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,116 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,117 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.202641 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,418 INFO codegen.CodeGenerator: Code generated in 235.00518 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,630 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:42843 in memory (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,635 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.216.44:39025 in memory (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:49,999 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,141 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,144 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,144 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,145 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,146 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,148 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,168 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,170 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,171 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.216.44:39025 (size: 35.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,172 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,174 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,175 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,182 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:50,209 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:42843 (size: 35.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,085 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 1905 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,086 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,087 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 1.936 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,088 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,088 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,089 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,089 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,167 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,170 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,170 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,170 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,170 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,173 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,188 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,190 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,191 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.216.44:39025 (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,191 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,191 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,191 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,194 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,210 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:42843 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,257 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,657 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 464 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,659 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.478 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,659 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,659 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,659 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,660 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.492399 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:52,719 INFO codegen.CodeGenerator: Code generated in 33.272584 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,017 INFO codegen.CodeGenerator: Code generated in 32.729617 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,129 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,130 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,130 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,131 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,132 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,134 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,152 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,154 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,155 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.216.44:39025 (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,156 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,156 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,156 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,158 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:53,176 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:42843 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,004 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1846 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,004 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,005 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.870 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,006 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,006 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,006 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.877712 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,555 INFO codegen.CodeGenerator: Code generated in 111.251318 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,567 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,567 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,567 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,567 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,568 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,569 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,573 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 77.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,574 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,575 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.216.44:39025 (size: 24.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,576 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,576 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,577 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,578 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,591 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:42843 (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,839 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 261 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,840 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,841 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.271 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,841 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,842 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,842 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:55,842 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,001 INFO codegen.CodeGenerator: Code generated in 77.69343 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,013 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,014 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,014 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,014 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,015 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,015 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,018 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,020 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,021 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.216.44:39025 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,023 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,024 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,024 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,026 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,043 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:42843 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,053 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,183 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 156 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,183 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,184 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.168 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,184 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,184 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,185 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.171776 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,300 INFO codegen.CodeGenerator: Code generated in 94.590963 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,308 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:42843 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,312 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.216.44:39025 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,324 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:42843 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,326 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.216.44:39025 in memory (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,348 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:42843 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,352 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.216.44:39025 in memory (size: 46.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,366 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.216.44:39025 in memory (size: 35.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,375 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:42843 in memory (size: 35.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,407 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:42843 in memory (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,415 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.216.44:39025 in memory (size: 24.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,449 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,452 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,453 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,453 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,453 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,454 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,456 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,469 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,476 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,477 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.216.44:39025 (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,478 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,478 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,479 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,480 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:56,498 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:42843 (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,045 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1565 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,045 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,046 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.589 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,047 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,047 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,047 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,047 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,047 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,051 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,054 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,056 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.216.44:39025 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,057 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,059 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,059 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,061 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,077 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:42843 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,086 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,132 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 72 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,132 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,133 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.085 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,134 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,134 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,134 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.685462 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,320 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,320 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,321 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,321 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,321 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,322 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,328 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,329 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,330 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.216.44:39025 (size: 28.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,330 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,330 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,330 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,332 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,341 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:42843 (size: 28.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,739 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 408 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,740 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,741 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.418 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,741 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,742 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,742 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,742 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,784 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,785 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,785 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,785 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,785 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,787 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,795 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.4 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,797 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,798 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.216.44:39025 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,799 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,799 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,800 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,801 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,818 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:42843 (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,827 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,902 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 101 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,902 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,903 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.114 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,906 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,906 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:58,906 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.122346 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,027 INFO codegen.CodeGenerator: Code generated in 11.629217 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,058 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,059 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,059 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,060 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,060 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,063 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,069 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 39.8 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,071 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,072 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.216.44:39025 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,073 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,073 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,073 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,075 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:16:59,086 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:42843 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,359 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 1284 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,359 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,360 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 1.296 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,361 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,361 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,361 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 1.302836 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,573 INFO codegen.CodeGenerator: Code generated in 47.31495 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,581 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,581 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,581 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,581 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,582 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,582 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,586 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 78.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,588 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,589 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.216.44:39025 (size: 25.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,589 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,590 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,590 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,591 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,606 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:42843 (size: 25.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,856 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,856 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,857 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.274 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,858 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,858 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,858 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:00,859 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,071 INFO codegen.CodeGenerator: Code generated in 90.789131 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,082 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,083 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,083 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,084 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,084 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,084 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,087 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 67.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,089 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,090 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.216.44:39025 (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,091 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,091 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,091 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,093 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,107 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:42843 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,112 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,183 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 90 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,184 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,185 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.098 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,186 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,189 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,190 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.107651 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,313 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,315 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,318 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,319 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,319 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,319 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,321 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,328 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,331 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,332 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.216.44:39025 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,335 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,336 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,337 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,341 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,355 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:42843 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,635 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 294 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,637 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,638 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.315 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,639 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,640 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,640 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,640 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,641 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,643 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,644 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,644 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.216.44:39025 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,645 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,645 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,646 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,647 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,656 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:42843 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,659 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,673 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,674 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,674 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.033 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,675 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,675 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,675 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.362290 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,770 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,770 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,770 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,770 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,771 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,771 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,775 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,779 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,780 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.216.44:39025 (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,780 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,781 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,781 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,782 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:01,794 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:42843 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,213 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 431 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,213 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,214 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.442 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,214 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,214 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,214 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,214 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,246 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,247 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,247 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,247 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,248 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,248 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,253 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,256 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,256 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.216.44:39025 (size: 46.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,257 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,257 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,258 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,259 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,269 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:42843 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,278 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,376 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 117 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,377 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,377 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.128 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,378 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,378 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,379 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.132125 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,450 INFO codegen.CodeGenerator: Code generated in 15.968148 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,515 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,516 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,516 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,516 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,523 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:42843 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,524 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,526 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,534 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.216.44:39025 in memory (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,565 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 39.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,567 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,568 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.216.44:39025 (size: 17.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,569 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,570 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,570 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,572 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,584 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:42843 (size: 17.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,628 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.216.44:39025 in memory (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,632 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:42843 in memory (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,664 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:42843 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,670 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.216.44:39025 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,691 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:42843 in memory (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,699 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.216.44:39025 in memory (size: 14.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,711 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:42843 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,712 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.216.44:39025 in memory (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,751 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.216.44:39025 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,752 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:42843 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,770 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.216.44:39025 in memory (size: 28.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,786 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:42843 in memory (size: 28.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,799 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.216.44:39025 in memory (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,801 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:42843 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,824 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:42843 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,848 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.216.44:39025 in memory (size: 46.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,858 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.216.44:39025 in memory (size: 25.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,865 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:42843 in memory (size: 25.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,880 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.216.44:39025 in memory (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:02,881 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:42843 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,453 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 881 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,453 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,454 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.926 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,454 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,454 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,455 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.939647 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,581 INFO codegen.CodeGenerator: Code generated in 35.23844 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,588 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,588 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,588 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,588 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,589 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,589 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,592 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 56.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,594 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,595 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.216.44:39025 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,595 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,597 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,597 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,598 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,609 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:42843 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,772 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 174 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,772 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,772 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.182 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,772 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,773 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,773 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,773 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,871 INFO codegen.CodeGenerator: Code generated in 52.198507 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,924 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,925 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,925 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,925 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,925 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,926 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,928 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 44.4 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,929 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,930 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.216.44:39025 (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,930 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,931 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,931 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,932 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,942 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:42843 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,946 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,998 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 66 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,998 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,999 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.072 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,999 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,999 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:03,999 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.074862 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,032 INFO codegen.CodeGenerator: Code generated in 19.602632 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,078 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,079 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,079 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,079 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,079 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,079 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,080 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,087 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.8 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,088 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,089 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.216.44:39025 (size: 14.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,089 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,090 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,090 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,091 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,105 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:42843 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,259 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 168 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,259 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,260 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.179 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,260 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,261 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,261 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,261 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,261 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,263 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,265 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,265 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.216.44:39025 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,266 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,266 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,266 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,268 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,304 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:42843 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,307 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,321 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,321 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,322 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.060 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,322 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,322 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,322 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.244585 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,458 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,459 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,459 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,459 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,459 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,459 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,463 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 85.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,464 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,465 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.216.44:39025 (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,465 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,465 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,465 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,467 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,476 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:42843 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,809 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 343 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,809 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,810 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.349 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,810 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,810 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,810 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,810 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,843 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,844 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,844 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,844 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,845 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,845 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,852 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 170.6 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,854 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,854 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.216.44:39025 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,855 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,855 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,855 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,857 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,868 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:42843 (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,876 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,997 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 141 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,997 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,998 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.152 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,998 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,998 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:04,999 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.155688 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,153 INFO codegen.CodeGenerator: Code generated in 28.629581 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,217 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,219 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,219 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,219 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,220 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,221 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,233 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 40.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,235 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,236 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.216.44:39025 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,238 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,238 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,239 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,240 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:05,259 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:42843 (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,627 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 1387 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,628 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,629 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 1.406 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,629 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,629 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,629 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 1.412443 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,967 INFO codegen.CodeGenerator: Code generated in 96.446091 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,973 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,973 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,973 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,973 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,974 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,974 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,978 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 77.3 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,979 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,980 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.216.44:39025 (size: 25.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,980 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,981 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,981 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,983 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:06,993 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:42843 (size: 25.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,248 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 266 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,249 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,250 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.274 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,251 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,252 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,252 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,253 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,416 INFO codegen.CodeGenerator: Code generated in 81.689118 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,443 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,444 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,444 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,444 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,444 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,445 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,447 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 67.1 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,448 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,450 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.216.44:39025 (size: 19.8 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,450 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,451 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,451 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,452 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,462 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:42843 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,466 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,528 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 76 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,529 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,529 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.084 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,530 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,530 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,531 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.086613 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,619 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.216.44:39025 in memory (size: 17.5 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,628 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:42843 in memory (size: 17.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,690 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:42843 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,697 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.216.44:39025 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,702 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,703 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,703 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,703 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,703 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,704 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,706 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,711 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:42843 in memory (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,721 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.216.44:39025 in memory (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,741 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.216.44:39025 in memory (size: 19.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,742 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,743 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,744 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.216.44:39025 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,744 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,745 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,745 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,746 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:42843 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,747 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,758 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.216.44:39025 in memory (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,760 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:42843 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,761 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:42843 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,770 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.216.44:39025 in memory (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,787 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:42843 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,798 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:42843 in memory (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,810 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.216.44:39025 in memory (size: 47.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,815 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:42843 in memory (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,822 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.216.44:39025 in memory (size: 14.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,825 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.216.44:39025 in memory (size: 25.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,827 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:42843 in memory (size: 25.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,833 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.216.44:39025 in memory (size: 14.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,834 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:42843 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,949 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 203 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,949 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.242 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,950 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,952 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,953 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,955 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.216.44:39025 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,956 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,957 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,957 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,959 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,969 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:42843 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,972 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,989 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,990 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,990 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.039 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,993 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,994 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:07,994 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.291876 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,118 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,119 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,119 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,119 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,119 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,119 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,122 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 75.4 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,123 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,124 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.216.44:39025 (size: 25.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,124 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,125 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,125 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,126 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,142 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:42843 (size: 25.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,533 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 407 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,533 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,534 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.414 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,534 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,534 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,534 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,534 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,562 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,563 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,563 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,563 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,563 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,563 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,569 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 144.2 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,570 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 41.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,571 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.216.44:39025 (size: 41.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,571 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,572 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,572 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,573 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,580 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:42843 (size: 41.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,587 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,651 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 77 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,651 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,652 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.088 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,652 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,652 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,653 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.091402 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,665 INFO codegen.CodeGenerator: Code generated in 9.064861 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,702 INFO codegen.CodeGenerator: Code generated in 7.253183 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,731 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,732 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,732 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,732 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,733 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,733 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,740 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 38.3 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,745 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,746 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.216.44:39025 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,748 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,749 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,749 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,750 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:08,761 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:42843 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,387 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 637 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,387 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,388 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.654 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,388 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,388 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,389 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.656992 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,483 INFO codegen.CodeGenerator: Code generated in 24.045467 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,490 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,491 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,491 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,491 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,492 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,493 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,498 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 45.9 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,500 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,503 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.216.44:39025 (size: 17.4 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,504 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,505 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,506 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,507 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,517 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:42843 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,638 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 131 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,638 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,641 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.145 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,641 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,641 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,642 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,642 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,701 INFO codegen.CodeGenerator: Code generated in 28.366139 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,723 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,728 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,728 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,728 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,729 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,729 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,731 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 33.2 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,733 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,733 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.216.44:39025 (size: 11.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,734 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,734 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,734 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,736 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,746 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:42843 (size: 11.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,750 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,782 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 47 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,783 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,783 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.053 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,784 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,784 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,784 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.057425 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,809 INFO codegen.CodeGenerator: Code generated in 20.4462 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,860 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,861 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,862 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,862 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,863 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,863 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,864 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,872 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 32.8 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,874 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,875 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.216.44:39025 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,875 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,876 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,876 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,877 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:09,887 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:42843 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,129 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 252 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,132 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,133 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.266 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,134 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,134 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,134 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,135 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,135 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,138 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,149 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,149 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.216.44:39025 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,150 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,150 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,151 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,152 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,160 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:42843 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,163 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,178 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,179 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,179 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,179 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,179 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,180 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.319152 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,375 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,400 INFO codegen.CodeGenerator: Code generated in 7.024763 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Registering RDD 191 (count at StatsGenerator.scala:66) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Got map stage job 32 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,406 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,409 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 24.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,411 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,411 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.216.44:39025 (size: 11.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,411 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,412 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,412 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,413 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,423 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:42843 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,467 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 54 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,467 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,478 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (count at StatsGenerator.scala:66) finished in 0.071 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,479 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,479 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,479 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,479 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,517 INFO codegen.CodeGenerator: Code generated in 20.719883 ms\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,527 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,529 INFO scheduler.DAGScheduler: Got job 33 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,529 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,529 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,529 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,530 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,532 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 11.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,534 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,537 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.216.44:39025 (size: 5.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,537 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,538 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,538 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,539 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,548 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:42843 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,551 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.216.44:56438\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,584 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 45 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,584 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,585 INFO scheduler.DAGScheduler: ResultStage 49 (count at StatsGenerator.scala:66) finished in 0.054 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,587 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,587 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:10,587 INFO scheduler.DAGScheduler: Job 33 finished: count at StatsGenerator.scala:66, took 0.058708 s\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,309 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,327 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,375 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,375 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,382 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,394 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,473 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,473 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,482 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,486 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,557 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,557 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,557 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,590 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,591 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-f41e6784-b566-4e46-a97d-d5f10dacdb7f\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,609 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-881e7685-a5cc-49eb-9ab1-67e8f6e0a9e7\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,693 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-06-11 02:17:11,694 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "Baseline generation complete\n"
     ]
    }
   ],
   "source": [
    "# Create model monitor object\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Define baseline input/output locations\n",
    "baseline_data_uri = f's3://{bucket}/{prefix}/cardio_engineered_clean.csv'\n",
    "baseline_results_uri = f's3://{bucket}/{prefix}/baseline-results'\n",
    "\n",
    "# Suggest baseline job\n",
    "baseline_job = monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format={'csv': {'header': True}},\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"Baseline generation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3fc0e-fd1f-4e0e-923a-4e325d564425",
   "metadata": {},
   "source": [
    "Full model monitoring pipeline has been prepared by first creating the SKLearn model object, which wraps the trained logistic regression model and inference script so SageMaker can use it. Then I enable data capture, which collects real-time input data and predictions as the model serves traffic, creating a record of live inference data. After that, I deploy the model as an endpoint to make it accessible for inference while capturing data. Once deployed, I generate a baseline using the fully cleaned and preprocessed training dataset; this allows SageMaker to compute statistics and constraints that define the models expected data distributions. These baseline statistics become the reference point that future data will be compared against to detect drift or anomalies. This full setup ensures that once monitoring jobs are configured, SageMaker can automatically evaluate incoming data for shifts that may impact model accuracy or stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636aeb4-cfe1-4999-96b1-97a027ad988e",
   "metadata": {},
   "source": [
    "* Calculates <b>statistics</b> (distribution, min, max, mean, std, percentiles, etc.)\n",
    "* Generates <b>constraints</b> (rules/thresholds learned from your dataset, e.g., feature X must be within certain boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285f5f8-e12c-40cd-b2c8-55a9876ee418",
   "metadata": {},
   "source": [
    "### Check Constraint Files Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78f73e78-2dc6-465c-92d4-7f2a261f186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline statistics and constraints found:\n",
      "cardio_data/baseline-results/constraints.json\n",
      "cardio_data/baseline-results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Your bucket and baseline folder\n",
    "bucket = 'sagemaker-us-east-1-531690656306'\n",
    "baseline_prefix = 'cardio_data/baseline-results/'\n",
    "\n",
    "# List objects in the baseline folder\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=baseline_prefix)\n",
    "\n",
    "# Check and print files\n",
    "if 'Contents' in response:\n",
    "    print(\"Baseline statistics and constraints found:\")\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"No baseline files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0392fe0e-bfe3-4536-8202-1627f6b1f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./cardio_model_monitoring.ipynb to s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_model_monitoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Save Notebook to S3\n",
    "!aws s3 cp cardio_model_monitoring.ipynb s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_model_monitoring.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643765fb-250d-4260-9c23-7499010bef28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## For Reference Only\n",
    "### <u>File Locations Summary for Model Monitoring Setup</u>\n",
    "\n",
    "#### Trained Model Artifact (used for deployment & endpoint)\n",
    "`s3://sagemaker-us-east-1-531690656306/model/logistic_model.tar.gz`\n",
    "\n",
    "#### Training Dataset (used for baseline generation)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_train.csv`\n",
    "\n",
    "#### Engineered Dataset (cleaned and engineered)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv`\n",
    "\n",
    "#### Baseline Results Output Folder (generated by model monitor baseline job)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/`\n",
    "\n",
    "#### Inside baseline-results:\n",
    "- `statistics.json`  \n",
    "  `s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/statistics.json`\n",
    "\n",
    "- `constraints.json`  \n",
    "  `s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/constraints.json`\n",
    "\n",
    "#### Data Capture Location (request/response payloads captured from endpoint)\n",
    "`s3://sagemaker-us-east-1-531690656306/data-capture/`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sagemaker-env)",
   "language": "python",
   "name": "sagemaker-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
