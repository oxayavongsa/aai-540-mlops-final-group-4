{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2d097e62-2546-47c0-9b15-31d2587e39f5",
   "metadata": {},
   "source": [
    "## Model Monitoring Setup\n",
    "Data Capture, Baseline Generation, Deployment Preparation for Model Quality Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9ebb3604-fada-4a50-8bfb-22161d040510",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports & Setup\n",
    "import boto3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import botocore\n",
    "from botocore.exceptions import ClientError\n",
    "\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role, Session\n",
    "from sagemaker.model_monitor import DataCaptureConfig, DefaultModelMonitor\n",
    "from sagemaker.sklearn.model import SKLearnModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8337e5d1-ffe5-4c91-b402-e59817e5622c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SageMaker session initialized\n"
     ]
    }
   ],
   "source": [
    "# Initialize session and role\n",
    "session = Session()\n",
    "role = get_execution_role()\n",
    "region = session.boto_region_name\n",
    "\n",
    "# Define bucket and paths\n",
    "bucket = 'sagemaker-us-east-1-531690656306'\n",
    "prefix = 'cardio_data'\n",
    "s3_client = boto3.client('s3', region_name=region)\n",
    "sagemaker_client = boto3.client('sagemaker', region_name=region)\n",
    "\n",
    "print(\"SageMaker session initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7f1151b2-b6b0-4bf2-9cc5-804ca8971ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   age  gender  height_ft  weight_lbs  systolic_bp  diastolic_bp  cholesterol  \\\n",
      "0   50       2       5.51      136.69          110            80            1   \n",
      "1   55       1       5.12      187.39          140            90            3   \n",
      "2   51       1       5.41      141.10          130            70            3   \n",
      "3   48       2       5.54      180.78          150           100            1   \n",
      "4   47       1       5.12      123.46          100            60            1   \n",
      "\n",
      "   gluc  smoke  alco  ...  cholesterol_label  pulse_pressure  chol_bmi_ratio  \\\n",
      "0     1      0     0  ...             Normal              30            4.55   \n",
      "1     1      0     0  ...  Well Above Normal              50            8.60   \n",
      "2     1      0     0  ...  Well Above Normal              60           12.74   \n",
      "3     1      0     0  ...             Normal              50            3.48   \n",
      "4     1      0     0  ...             Normal              40            4.35   \n",
      "\n",
      "  height_in age_years  is_hypertensive  bp_category  bmi_category  \\\n",
      "0     66.12        50                0       stage1        normal   \n",
      "1     61.44        55                1       stage2         obese   \n",
      "2     64.92        51                0       stage1        normal   \n",
      "3     66.48        48                1       stage2    overweight   \n",
      "4     61.44        47                0       normal        normal   \n",
      "\n",
      "   age_gluc_interaction  lifestyle_score  \n",
      "0                    50               -1  \n",
      "1                    55               -1  \n",
      "2                    51                0  \n",
      "3                    48               -1  \n",
      "4                    47                0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Cleaned dataset uploaded to S3'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load your existing engineered dataset\n",
    "df = pd.read_csv('cardio_engineered.csv')\n",
    "\n",
    "# Check data\n",
    "print(df.head())\n",
    "\n",
    "# Save cleaned CSV\n",
    "clean_file = 'cardio_engineered_clean.csv'\n",
    "df.to_csv(clean_file, index=False, encoding='utf-8-sig')\n",
    "\n",
    "# Upload cleaned CSV to S3\n",
    "s3_client.upload_file(clean_file, bucket, f'{prefix}/cardio_engineered_clean.csv')\n",
    "\"Cleaned dataset uploaded to S3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a98909a8-e882-474f-baf3-a7011645f15f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model and inference script\n",
    "model_artifact = f's3://{bucket}/model/logistic_model.tar.gz'\n",
    "entry_point = 'inference.py'\n",
    "endpoint_name = 'cardio-logistic-monitor-endpoint'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "86282eab-2f3a-4d26-8bba-f2ff0c9bfdb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model object\n",
    "sklearn_model = SKLearnModel(\n",
    "    model_data=model_artifact,\n",
    "    role=role,\n",
    "    entry_point=entry_point,\n",
    "    framework_version='0.23-1',\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Enable full data capture\n",
    "data_capture_config = DataCaptureConfig(\n",
    "    enable_capture=True,\n",
    "    sampling_percentage=100,\n",
    "    destination_s3_uri=f's3://{bucket}/data-capture',\n",
    "    capture_options=['Request', 'Response']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "d447eb66-fcaf-4be7-840e-3bcaebc6f0ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint 'cardio-logistic-monitor-endpoint' already exists. Skipping deployment.\n"
     ]
    }
   ],
   "source": [
    "# Deploy only if endpoint doesn't exist\n",
    "def deploy_if_not_exists(model, endpoint_name, instance_type, data_capture_config):\n",
    "    try:\n",
    "        sagemaker_client.describe_endpoint(EndpointName=endpoint_name)\n",
    "        print(f\"Endpoint '{endpoint_name}' already exists. Skipping deployment.\")\n",
    "    except sagemaker_client.exceptions.ClientError as e:\n",
    "        if 'Could not find endpoint' in str(e):\n",
    "            model.deploy(\n",
    "                initial_instance_count=1,\n",
    "                instance_type=instance_type,\n",
    "                endpoint_name=endpoint_name,\n",
    "                data_capture_config=data_capture_config\n",
    "            )\n",
    "            print(f\"Deployed endpoint '{endpoint_name}' successfully.\")\n",
    "        else:\n",
    "            raise\n",
    "\n",
    "# Call deployment function\n",
    "deploy_if_not_exists(\n",
    "    model=sklearn_model,\n",
    "    endpoint_name=endpoint_name,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    data_capture_config=data_capture_config\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "6983fe45-b4c2-4220-ab74-65cd97b0869c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Job Name:  baseline-suggestion-job-2025-06-09-08-45-44-594\n",
      "Inputs:  [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv', 'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3DataDistributionType': 'FullyReplicated', 'S3CompressionType': 'None'}}]\n",
      "Outputs:  [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results', 'LocalPath': '/opt/ml/processing/output', 'S3UploadMode': 'EndOfJob'}}]\n",
      "...............\u001b[34m2025-06-09 08:47:56.993732: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-09 08:47:56.993761: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:47:58.639008: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\u001b[0m\n",
      "\u001b[34m2025-06-09 08:47:58.639036: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:47:58.639058: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-10-0-79-62.ec2.internal): /proc/driver/nvidia/version does not exist\u001b[0m\n",
      "\u001b[34m2025-06-09 08:47:58.639343: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA\u001b[0m\n",
      "\u001b[34mTo enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,255 - __main__ - INFO - All params:{'ProcessingJobArn': 'arn:aws:sagemaker:us-east-1:531690656306:processing-job/baseline-suggestion-job-2025-06-09-08-45-44-594', 'ProcessingJobName': 'baseline-suggestion-job-2025-06-09-08-45-44-594', 'Environment': {'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}, 'AppSpecification': {'ImageUri': '156813124566.dkr.ecr.us-east-1.amazonaws.com/sagemaker-model-monitor-analyzer', 'ContainerEntrypoint': None, 'ContainerArguments': None}, 'ProcessingInputs': [{'InputName': 'baseline_dataset_input', 'AppManaged': False, 'S3Input': {'LocalPath': '/opt/ml/processing/input/baseline_dataset_input', 'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv', 'S3DataDistributionType': 'FullyReplicated', 'S3DataType': 'S3Prefix', 'S3InputMode': 'File', 'S3CompressionType': 'None', 'S3DownloadMode': 'StartOfJob'}, 'DatasetDefinitionInput': None}], 'ProcessingOutputConfig': {'Outputs': [{'OutputName': 'monitoring_output', 'AppManaged': False, 'S3Output': {'LocalPath': '/opt/ml/processing/output', 'S3Uri': 's3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results', 'S3UploadMode': 'EndOfJob'}, 'FeatureStoreOutput': None}], 'KmsKeyId': None}, 'ProcessingResources': {'ClusterConfig': {'InstanceCount': 1, 'InstanceType': 'ml.m5.xlarge', 'VolumeSizeInGB': 20, 'VolumeKmsKeyId': None}}, 'NetworkConfig': {'VpcConfig': None, 'EnableNetworkIsolation': False, 'EnableInterContainerTrafficEncryption': False}, 'RoleArn': 'arn:aws:iam::531690656306:role/LabRole', 'StoppingCondition': {'MaxRuntimeInSeconds': 3600}}\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,255 - __main__ - INFO - Current Environment:{'dataset_format': '{\"csv\": {\"header\": true}}', 'dataset_source': '/opt/ml/processing/input/baseline_dataset_input', 'output_path': '/opt/ml/processing/output', 'publish_cloudwatch_metrics': 'Disabled'}\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,255 - __main__ - INFO - categorical_drift_method:None\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,256 - DefaultDataAnalyzer - INFO - Performing analysis with input: {\"dataset_source\": \"/opt/ml/processing/input/baseline_dataset_input\", \"dataset_format\": {\"csv\": {\"header\": true}}, \"output_path\": \"/opt/ml/processing/output\", \"monitoring_input_type\": null, \"analysis_type\": null, \"problem_type\": null, \"inference_attribute\": null, \"probability_attribute\": null, \"ground_truth_attribute\": null, \"probability_threshold_attribute\": null, \"positive_label\": null, \"exclude_features_attribute\": null, \"record_preprocessor_script\": null, \"post_analytics_processor_script\": null, \"baseline_constraints\": null, \"baseline_statistics\": null, \"data_quality_monitoring_config\": {\"evaluate_constraints\": \"Enabled\", \"emit_metrics\": \"Enabled\", \"datatype_check_threshold\": 1.0, \"domain_content_threshold\": 1.0, \"distribution_constraints\": {\"perform_comparison\": \"Enabled\", \"comparison_threshold\": 0.1, \"comparison_method\": \"Robust\", \"categorical_comparison_threshold\": 0.1, \"categorical_drift_method\": \"LInfinity\"}}, \"start_time\": null, \"end_time\": null, \"metric_time\": null, \"cloudwatch_metrics_directory\": \"/opt/ml/output/metrics/cloudwatch\", \"publish_cloudwatch_metrics\": \"Disabled\", \"sagemaker_endpoint_name\": null, \"sagemaker_monitoring_schedule_name\": null, \"output_message_file\": \"/opt/ml/output/message\", \"detect_outliers\": null, \"detect_drift\": null, \"image_data\": null, \"report_enabled\": false, \"auto_ml_job_detail\": null}\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,256 - DefaultDataAnalyzer - INFO - Bootstrapping yarn\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,256 - bootstrap - INFO - Copy aws jars\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,319 - bootstrap - INFO - Copy cluster config\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,320 - bootstrap - INFO - Write runtime cluster config\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,320 - bootstrap - INFO - Resource Config is: {'current_host': 'algo-1', 'current_instance_type': 'ml.m5.xlarge', 'current_group_name': 'homogeneousCluster', 'hosts': ['algo-1'], 'instance_groups': [{'instance_group_name': 'homogeneousCluster', 'instance_type': 'ml.m5.xlarge', 'hosts': ['algo-1']}], 'network_interface_name': 'eth0', 'topology': None}\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,328 - bootstrap - INFO - Finished Yarn configuration files setup.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,328 - bootstrap - INFO - Starting spark process for master node algo-1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,328 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs namenode -format -force\u001b[0m\n",
      "\u001b[34mWARNING: /usr/hadoop-3.0.0/logs does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,813 INFO namenode.NameNode: STARTUP_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG: Starting NameNode\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   host = algo-1/10.0.79.62\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   args = [-format, -force]\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   version = 3.0.0\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   classpath = /usr/hadoop-3.0.0/etc/hadoop:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsp-api-2.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/junit-4.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-log4j12-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hamcrest-core-1.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jul-to-slf4j-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/mockito-all-1.8.5.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/slf4j-api-1.7.25.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/hadoop-aws-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/lib/aws-java-sdk-bundle-1.11.199.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/common/hadoop-kms-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsch-0.1.54.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-core-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-math3-3.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jettison-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-json-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-core-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-logging-1.1.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-pkix-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-http-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/snappy-java-1.0.5.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-admin-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-all-4.0.23.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-daemon-1.0.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/avro-1.7.7.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-servlet-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-xml-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/accessors-smart-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-compress-1.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-framework-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang3-3.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okio-1.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/javax.servlet-api-3.1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-beanutils-1.9.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-common-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-configuration2-2.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-xdr-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-mapper-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-jaxrs-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-io-2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/woodstox-core-5.0.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/re2j-1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/token-provider-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/leveldbjni-all-1.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-client-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/paranamer-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/log4j-1.2.17.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-server-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-impl-2.2.3-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-identity-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpcore-4.4.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/gson-2.2.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/netty-3.10.5.Final.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-recipes-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jaxb-api-2.2.11.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/zookeeper-3.4.9.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-databind-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-cli-1.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-auth-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/nimbus-jose-jwt-4.41.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-servlet-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-server-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-crypto-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerb-simplekdc-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/okhttp-2.4.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/xz-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-xc-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-asn1-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr305-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jcip-annotations-1.0-1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-config-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/hadoop-annotations-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/stax2-api-3.1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-net-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jersey-server-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-lang-2.6.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-webapp-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jsr311-api-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-security-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/kerby-util-1.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-codec-1.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/commons-collections-3.2.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-simple-1.1.1.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-io-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/htrace-core4-4.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/guava-11.0.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jackson-core-asl-1.9.13.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/json-smart-2.3.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/protobuf-java-2.5.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/httpclient-4.5.2.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/asm-5.0.4.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/jetty-util-ajax-9.3.19.v20170502.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/lib/curator-client-2.12.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-httpfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-nfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/hdfs/hadoop-hdfs-native-client-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-app-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-jobclient-3.0.0-tests.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-core-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-nativetask-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-shuffle-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/mapreduce/hadoop-mapreduce-client-hs-plugins-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop2-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-el-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/fst-2.50.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/HikariCP-java7-2.4.12.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/java-util-1.9.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-common-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-procedure-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-client-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-servlet-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jersey-guice-1.19.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-prefix-tree-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/findbugs-annotations-1.3.9-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-2.2.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/joni-2.1.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-server-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/javax.inject-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-base-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-client-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jamon-runtime-2.4.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-protocol-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/htrace-core-3.1.0-incubating.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/guice-4.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/disruptor-3.3.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jcodings-1.0.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/aopalliance-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-compiler-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-csv-1.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-math-2.2.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-jaxrs-json-provider-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/json-io-2.5.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jasper-runtime-5.5.23.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/metrics-core-3.0.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jackson-module-jaxb-annotations-2.7.8.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/geronimo-jcache_1.0_spec-1.0-alpha-1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-annotations-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/commons-httpclient-3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/hbase-hadoop-compat-1.2.6.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/ehcache-3.3.1.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/mssql-jdbc-6.2.1.jre7.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/jsp-api-2.1-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/lib/servlet-api-2.5-6.1.14.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-hbase-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-resourcemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-unmanaged-am-launcher-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-common-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-web-proxy-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-router-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop\u001b[0m\n",
      "\u001b[34m-yarn-server-timelineservice-hbase-tests-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-registry-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-applications-distributedshell-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-nodemanager-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timeline-pluginstorage-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-timelineservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-applicationhistoryservice-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-client-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-api-3.0.0.jar:/usr/hadoop-3.0.0/share/hadoop/yarn/hadoop-yarn-server-sharedcachemanager-3.0.0.jar\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   build = https://git-wip-us.apache.org/repos/asf/hadoop.git -r c25427ceca461ee979d30edd7a4b0f50718e6533; compiled by 'andrew' on 2017-12-08T19:16Z\u001b[0m\n",
      "\u001b[34mSTARTUP_MSG:   java = 1.8.0_392\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,821 INFO namenode.NameNode: registered UNIX signal handlers for [TERM, HUP, INT]\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:00,826 INFO namenode.NameNode: createNameNode [-format, -force]\u001b[0m\n",
      "\u001b[34mFormatting using clusterid: CID-a03c9890-35b6-4f63-9c66-ad98b5ff7732\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,360 INFO namenode.FSEditLog: Edit logging is async:true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,376 INFO namenode.FSNamesystem: KeyProvider: null\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,377 INFO namenode.FSNamesystem: fsLock is fair: true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,380 INFO namenode.FSNamesystem: Detailed lock hold time metrics enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,386 INFO namenode.FSNamesystem: fsOwner             = root (auth:SIMPLE)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,386 INFO namenode.FSNamesystem: supergroup          = supergroup\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,386 INFO namenode.FSNamesystem: isPermissionEnabled = true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,386 INFO namenode.FSNamesystem: HA Enabled: false\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,430 INFO common.Util: dfs.datanode.fileio.profiling.sampling.percentage set to 0. Disabling file IO profiling\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,445 INFO blockmanagement.DatanodeManager: dfs.block.invalidate.limit: configured=1000, counted=60, effected=1000\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,446 INFO blockmanagement.DatanodeManager: dfs.namenode.datanode.registration.ip-hostname-check=true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,451 INFO blockmanagement.BlockManager: dfs.namenode.startup.delay.block.deletion.sec is set to 000:00:00:00.000\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,454 INFO blockmanagement.BlockManager: The block deletion will start around 2025 Jun 09 08:48:01\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,456 INFO util.GSet: Computing capacity for map BlocksMap\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,456 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,458 INFO util.GSet: 2.0% max memory 3.1 GB = 63.1 MB\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,458 INFO util.GSet: capacity      = 2^23 = 8388608 entries\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,494 INFO blockmanagement.BlockManager: dfs.block.access.token.enable = false\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO Configuration.deprecation: No unit for dfs.namenode.safemode.extension(30000) assuming MILLISECONDS\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.threshold-pct = 0.9990000128746033\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.min.datanodes = 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManagerSafeMode: dfs.namenode.safemode.extension = 30000\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: defaultReplication         = 3\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: maxReplication             = 512\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: minReplication             = 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: maxReplicationStreams      = 2\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: redundancyRecheckInterval  = 3000ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: encryptDataTransfer        = false\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,498 INFO blockmanagement.BlockManager: maxNumBlocksToLog          = 1000\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,524 INFO util.GSet: Computing capacity for map INodeMap\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,524 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,525 INFO util.GSet: 1.0% max memory 3.1 GB = 31.5 MB\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,525 INFO util.GSet: capacity      = 2^22 = 4194304 entries\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,526 INFO namenode.FSDirectory: ACLs enabled? false\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,526 INFO namenode.FSDirectory: POSIX ACL inheritance enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,526 INFO namenode.FSDirectory: XAttrs enabled? true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,527 INFO namenode.NameNode: Caching file names occurring more than 10 times\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,531 INFO snapshot.SnapshotManager: Loaded config captureOpenFiles: false, skipCaptureAccessTimeOnlyChange: false, snapshotDiffAllowSnapRootDescendant: true\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,535 INFO util.GSet: Computing capacity for map cachedBlocks\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,535 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,535 INFO util.GSet: 0.25% max memory 3.1 GB = 7.9 MB\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,535 INFO util.GSet: capacity      = 2^20 = 1048576 entries\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,572 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.window.num.buckets = 10\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,572 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.num.users = 10\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,572 INFO metrics.TopMetrics: NNTop conf: dfs.namenode.top.windows.minutes = 1,5,25\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,575 INFO namenode.FSNamesystem: Retry cache on namenode is enabled\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,575 INFO namenode.FSNamesystem: Retry cache will use 0.03 of total heap and retry cache entry expiry time is 600000 millis\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,576 INFO util.GSet: Computing capacity for map NameNodeRetryCache\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,577 INFO util.GSet: VM type       = 64-bit\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,577 INFO util.GSet: 0.029999999329447746% max memory 3.1 GB = 968.9 KB\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,577 INFO util.GSet: capacity      = 2^17 = 131072 entries\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,598 INFO namenode.FSImage: Allocated new BlockPoolId: BP-1619057240-10.0.79.62-1749458881592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,612 INFO common.Storage: Storage directory /opt/amazon/hadoop/hdfs/namenode has been successfully formatted.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,619 INFO namenode.FSImageFormatProtobuf: Saving image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 using no compression\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,701 INFO namenode.FSImageFormatProtobuf: Image file /opt/amazon/hadoop/hdfs/namenode/current/fsimage.ckpt_0000000000000000000 of size 389 bytes saved in 0 seconds.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,713 INFO namenode.NNStorageRetentionManager: Going to retain 1 images with txid >= 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,717 INFO namenode.NameNode: SHUTDOWN_MSG: \u001b[0m\n",
      "\u001b[34m/************************************************************\u001b[0m\n",
      "\u001b[34mSHUTDOWN_MSG: Shutting down NameNode at algo-1/10.0.79.62\u001b[0m\n",
      "\u001b[34m************************************************************/\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:01,729 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:03,789 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start namenode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:03,790 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:05,871 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/hdfs --daemon start datanode, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:05,871 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34mWARNING: /var/log/yarn/ does not exist. Creating.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:07,944 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start resourcemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:07,944 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:10,029 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start nodemanager, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:10,030 - bootstrap - INFO - Running command: /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver\u001b[0m\n",
      "\u001b[34mWARNING: YARN_LOG_DIR has been replaced by HADOOP_LOG_DIR. Using value of YARN_LOG_DIR.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:12,164 - bootstrap - INFO - Failed to run /usr/hadoop-3.0.0/bin/yarn --daemon start proxyserver, return code 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:12,165 - DefaultDataAnalyzer - INFO - Total number of hosts in the cluster: 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:22,172 - DefaultDataAnalyzer - INFO - Running command: bin/spark-submit --master yarn --deploy-mode client --conf spark.hadoop.fs.s3a.aws.credentials.provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider --conf spark.serializer=org.apache.spark.serializer.KryoSerializer /opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:23,968 WARN util.NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:24,395 INFO Main: Start analyzing with args: --analytics_input /tmp/spark_job_config.json\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:24,433 INFO Main: Analytics input path: DataAnalyzerParams(/tmp/spark_job_config.json,yarn)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:24,447 INFO FileUtil: Read file from path /tmp/spark_job_config.json.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,017 INFO spark.SparkContext: Running Spark version 3.3.0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,043 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,043 INFO resource.ResourceUtils: No custom resources configured for spark.driver.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,043 INFO resource.ResourceUtils: ==============================================================\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,053 INFO spark.SparkContext: Submitted application: SageMakerDataAnalyzer\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,079 INFO resource.ResourceProfile: Default ResourceProfile created, executor resources: Map(cores -> name: cores, amount: 3, script: , vendor: , memory -> name: memory, amount: 11507, script: , vendor: , offHeap -> name: offHeap, amount: 0, script: , vendor: ), task resources: Map(cpus -> name: cpus, amount: 1.0)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,093 INFO resource.ResourceProfile: Limiting resource is cpus at 3 tasks per executor\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,095 INFO resource.ResourceProfileManager: Added ResourceProfile id: 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,146 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,146 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,147 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,147 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,147 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,498 INFO util.Utils: Successfully started service 'sparkDriver' on port 44489.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,536 INFO spark.SparkEnv: Registering MapOutputTracker\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,582 INFO spark.SparkEnv: Registering BlockManagerMaster\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,601 INFO storage.BlockManagerMasterEndpoint: Using org.apache.spark.storage.DefaultTopologyMapper for getting topology information\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,601 INFO storage.BlockManagerMasterEndpoint: BlockManagerMasterEndpoint up\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,637 INFO spark.SparkEnv: Registering BlockManagerMasterHeartbeat\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,660 INFO storage.DiskBlockManager: Created local directory at /tmp/blockmgr-7ca8cb76-6b05-4b11-aedf-cafe35dd5ff3\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,677 INFO memory.MemoryStore: MemoryStore started with capacity 1458.6 MiB\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,717 INFO spark.SparkEnv: Registering OutputCommitCoordinator\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:25,752 INFO spark.SparkContext: Added JAR file:/opt/amazon/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar at spark://10.0.79.62:44489/jars/sagemaker-data-analyzer-1.0-jar-with-dependencies.jar with timestamp 1749458905011\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:26,342 INFO client.RMProxy: Connecting to ResourceManager at /10.0.79.62:8032\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,096 INFO conf.Configuration: resource-types.xml not found\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,097 INFO resource.ResourceUtils: Unable to find 'resource-types.xml'.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,104 INFO yarn.Client: Verifying our application has not requested more than the maximum memory capability of the cluster (15692 MB per container)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,104 INFO yarn.Client: Will allocate AM container, with 896 MB memory including 384 MB overhead\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,104 INFO yarn.Client: Setting up container launch context for our AM\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,105 INFO yarn.Client: Setting up the launch environment for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,111 INFO yarn.Client: Preparing resources for our AM container\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:27,194 WARN yarn.Client: Neither spark.yarn.jars nor spark.yarn.archive is set, falling back to uploading libraries under SPARK_HOME.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:29,476 INFO yarn.Client: Uploading resource file:/tmp/spark-d60aa5d2-6b1f-48db-bc0a-2c62cd58f254/__spark_libs__53737575255321262.zip -> hdfs://10.0.79.62/user/root/.sparkStaging/application_1749458887492_0001/__spark_libs__53737575255321262.zip\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:30,911 INFO yarn.Client: Uploading resource file:/tmp/spark-d60aa5d2-6b1f-48db-bc0a-2c62cd58f254/__spark_conf__7143907457972879302.zip -> hdfs://10.0.79.62/user/root/.sparkStaging/application_1749458887492_0001/__spark_conf__.zip\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,363 INFO spark.SecurityManager: Changing view acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,363 INFO spark.SecurityManager: Changing modify acls to: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,363 INFO spark.SecurityManager: Changing view acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,364 INFO spark.SecurityManager: Changing modify acls groups to: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,364 INFO spark.SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users  with view permissions: Set(root); groups with view permissions: Set(); users  with modify permissions: Set(root); groups with modify permissions: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,393 INFO yarn.Client: Submitting application application_1749458887492_0001 to ResourceManager\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:31,608 INFO impl.YarnClientImpl: Submitted application application_1749458887492_0001\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:32,626 INFO yarn.Client: Application report for application_1749458887492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:32,630 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: [Mon Jun 09 08:48:32 +0000 2025] Scheduler has assigned a container for AM, waiting for AM container to be launched\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1749458911497\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1749458887492_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:33,634 INFO yarn.Client: Application report for application_1749458887492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:34,637 INFO yarn.Client: Application report for application_1749458887492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:35,640 INFO yarn.Client: Application report for application_1749458887492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:36,647 INFO yarn.Client: Application report for application_1749458887492_0001 (state: ACCEPTED)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,553 INFO cluster.YarnClientSchedulerBackend: Add WebUI Filter. org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter, Map(PROXY_HOSTS -> algo-1, PROXY_URI_BASES -> http://algo-1:8088/proxy/application_1749458887492_0001), /proxy/application_1749458887492_0001\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,651 INFO yarn.Client: Application report for application_1749458887492_0001 (state: RUNNING)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,651 INFO yarn.Client: \u001b[0m\n",
      "\u001b[34m#011 client token: N/A\u001b[0m\n",
      "\u001b[34m#011 diagnostics: N/A\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster host: 10.0.79.62\u001b[0m\n",
      "\u001b[34m#011 ApplicationMaster RPC port: -1\u001b[0m\n",
      "\u001b[34m#011 queue: default\u001b[0m\n",
      "\u001b[34m#011 start time: 1749458911497\u001b[0m\n",
      "\u001b[34m#011 final status: UNDEFINED\u001b[0m\n",
      "\u001b[34m#011 tracking URL: http://algo-1:8088/proxy/application_1749458887492_0001/\u001b[0m\n",
      "\u001b[34m#011 user: root\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,653 INFO cluster.YarnClientSchedulerBackend: Application application_1749458887492_0001 has started running.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,663 INFO util.Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 35505.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,663 INFO netty.NettyBlockTransferService: Server created on 10.0.79.62:35505\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,665 INFO storage.BlockManager: Using org.apache.spark.storage.RandomBlockReplicationPolicy for block replication policy\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,676 INFO storage.BlockManagerMaster: Registering BlockManager BlockManagerId(driver, 10.0.79.62, 35505, None)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,680 INFO storage.BlockManagerMasterEndpoint: Registering block manager 10.0.79.62:35505 with 1458.6 MiB RAM, BlockManagerId(driver, 10.0.79.62, 35505, None)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,684 INFO storage.BlockManagerMaster: Registered BlockManager BlockManagerId(driver, 10.0.79.62, 35505, None)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,685 INFO storage.BlockManager: Initialized BlockManager: BlockManagerId(driver, 10.0.79.62, 35505, None)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:37,852 INFO util.log: Logging initialized @15455ms to org.sparkproject.jetty.util.log.Slf4jLog\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:38,936 INFO cluster.YarnSchedulerBackend$YarnSchedulerEndpoint: ApplicationMaster registered as NettyRpcEndpointRef(spark-client://YarnAM)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:42,754 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Registered executor NettyRpcEndpointRef(spark-client://Executor) (10.0.79.62:51980) with ID 1,  ResourceProfileId 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:42,963 INFO storage.BlockManagerMasterEndpoint: Registering block manager algo-1:41655 with 5.8 GiB RAM, BlockManagerId(1, algo-1, 41655, None)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:56,226 INFO cluster.YarnClientSchedulerBackend: SchedulerBackend is ready for scheduling beginning after waiting maxRegisteredResourcesWaitingTime: 30000000000(ns)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:56,424 WARN spark.SparkContext: Spark is not running in local mode, therefore the checkpoint directory must not be on the local filesystem. Directory '/tmp' appears to be on the local filesystem.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:56,480 INFO internal.SharedState: Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:56,485 INFO internal.SharedState: Warehouse path is 'file:/usr/spark-3.3.0/spark-warehouse'.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:57,431 INFO datasources.InMemoryFileIndex: It took 39 ms to list leaf files for 1 paths.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:57,614 INFO memory.MemoryStore: Block broadcast_0 stored as values in memory (estimated size 416.9 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:57,947 INFO memory.MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 39.3 KiB, free 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:57,950 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on 10.0.79.62:35505 (size: 39.3 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:57,955 INFO spark.SparkContext: Created broadcast 0 from csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,321 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,324 INFO input.FileInputFormat: Total input files to process : 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,329 INFO input.CombineFileInputFormat: DEBUG: Terminated node allocation with : CompletedNodes: 1, size left: 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,393 INFO spark.SparkContext: Starting job: csv at DatasetReader.scala:99\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,414 INFO scheduler.DAGScheduler: Got job 0 (csv at DatasetReader.scala:99) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,414 INFO scheduler.DAGScheduler: Final stage: ResultStage 0 (csv at DatasetReader.scala:99)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,415 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,416 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,420 INFO scheduler.DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,450 INFO memory.MemoryStore: Block broadcast_1 stored as values in memory (estimated size 7.3 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,458 INFO memory.MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 4.2 KiB, free 1458.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,459 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on 10.0.79.62:35505 (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,461 INFO spark.SparkContext: Created broadcast 1 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,488 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[2] at csv at DatasetReader.scala:99) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,490 INFO cluster.YarnScheduler: Adding task set 0.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,540 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4636 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:58,769 INFO storage.BlockManagerInfo: Added broadcast_1_piece0 in memory on algo-1:41655 (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,602 INFO storage.BlockManagerInfo: Added broadcast_0_piece0 in memory on algo-1:41655 (size: 39.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,961 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 1436 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,963 INFO cluster.YarnScheduler: Removed TaskSet 0.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,970 INFO scheduler.DAGScheduler: ResultStage 0 (csv at DatasetReader.scala:99) finished in 1.527 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,974 INFO scheduler.DAGScheduler: Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,974 INFO cluster.YarnScheduler: Killing all running tasks in stage 0: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:48:59,976 INFO scheduler.DAGScheduler: Job 0 finished: csv at DatasetReader.scala:99, took 1.583155 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:00,188 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on algo-1:41655 in memory (size: 4.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:00,192 INFO storage.BlockManagerInfo: Removed broadcast_1_piece0 on 10.0.79.62:35505 in memory (size: 4.2 KiB, free: 1458.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,433 INFO datasources.FileSourceStrategy: Pushed Filters: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,435 INFO datasources.FileSourceStrategy: Post-Scan Filters: \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,438 INFO datasources.FileSourceStrategy: Output Data Schema: struct<age: string, gender: string, height_ft: string, weight_lbs: string, systolic_bp: string ... 22 more fields>\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,632 INFO memory.MemoryStore: Block broadcast_2 stored as values in memory (estimated size 416.5 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,647 INFO memory.MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 39.2 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,648 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on 10.0.79.62:35505 (size: 39.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,653 INFO spark.SparkContext: Created broadcast 2 from head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,666 INFO execution.FileSourceScanExec: Planning scan with bin packing, max size: 4194304 bytes, open cost is considered as scanning 4194304 bytes.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,714 INFO spark.SparkContext: Starting job: head at DataAnalyzer.scala:124\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,715 INFO scheduler.DAGScheduler: Got job 1 (head at DataAnalyzer.scala:124) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,715 INFO scheduler.DAGScheduler: Final stage: ResultStage 1 (head at DataAnalyzer.scala:124)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,716 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,719 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,725 INFO scheduler.DAGScheduler: Submitting ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,785 INFO memory.MemoryStore: Block broadcast_3 stored as values in memory (estimated size 19.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,788 INFO memory.MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 8.9 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,789 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on 10.0.79.62:35505 (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,789 INFO spark.SparkContext: Created broadcast 3 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,790 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (MapPartitionsRDD[11] at head at DataAnalyzer.scala:124) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,790 INFO cluster.YarnScheduler: Adding task set 1.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,794 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:02,846 INFO storage.BlockManagerInfo: Added broadcast_3_piece0 in memory on algo-1:41655 (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:03,744 INFO storage.BlockManagerInfo: Added broadcast_2_piece0 in memory on algo-1:41655 (size: 39.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:04,914 INFO storage.BlockManagerInfo: Added rdd_7_0 in memory on algo-1:41655 (size: 3.2 MiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,030 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 2238 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,031 INFO cluster.YarnScheduler: Removed TaskSet 1.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,031 INFO scheduler.DAGScheduler: ResultStage 1 (head at DataAnalyzer.scala:124) finished in 2.301 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,032 INFO scheduler.DAGScheduler: Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,032 INFO cluster.YarnScheduler: Killing all running tasks in stage 1: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,033 INFO scheduler.DAGScheduler: Job 1 finished: head at DataAnalyzer.scala:124, took 2.318884 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,311 INFO codegen.CodeGenerator: Code generated in 205.866354 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,579 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on 10.0.79.62:35505 in memory (size: 8.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,582 INFO storage.BlockManagerInfo: Removed broadcast_3_piece0 on algo-1:41655 in memory (size: 8.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:05,888 WARN util.package: Truncated the string representation of a plan since it was too large. This behavior can be adjusted by setting 'spark.sql.debug.maxToStringFields'.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,018 INFO scheduler.DAGScheduler: Registering RDD 16 (collect at AnalysisRunner.scala:326) as input to shuffle 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,021 INFO scheduler.DAGScheduler: Got map stage job 2 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,022 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,022 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,025 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,027 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,052 INFO memory.MemoryStore: Block broadcast_4 stored as values in memory (estimated size 116.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,054 INFO memory.MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 35.3 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,055 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on 10.0.79.62:35505 (size: 35.3 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,056 INFO spark.SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,058 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[16] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,058 INFO cluster.YarnScheduler: Adding task set 2.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,068 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:06,109 INFO storage.BlockManagerInfo: Added broadcast_4_piece0 in memory on algo-1:41655 (size: 35.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,121 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 2055 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,121 INFO cluster.YarnScheduler: Removed TaskSet 2.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,123 INFO scheduler.DAGScheduler: ShuffleMapStage 2 (collect at AnalysisRunner.scala:326) finished in 2.093 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,123 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,124 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,124 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,125 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,201 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,203 INFO scheduler.DAGScheduler: Got job 3 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,204 INFO scheduler.DAGScheduler: Final stage: ResultStage 4 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,204 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,204 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,205 INFO scheduler.DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,219 INFO memory.MemoryStore: Block broadcast_5 stored as values in memory (estimated size 169.3 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,221 INFO memory.MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 46.7 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,221 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on 10.0.79.62:35505 (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,222 INFO spark.SparkContext: Created broadcast 5 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,223 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[19] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,223 INFO cluster.YarnScheduler: Adding task set 4.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,226 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 4.0 (TID 3) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,243 INFO storage.BlockManagerInfo: Added broadcast_5_piece0 in memory on algo-1:41655 (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,286 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 0 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,649 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 4.0 (TID 3) in 424 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,649 INFO cluster.YarnScheduler: Removed TaskSet 4.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,651 INFO scheduler.DAGScheduler: ResultStage 4 (collect at AnalysisRunner.scala:326) finished in 0.438 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,651 INFO scheduler.DAGScheduler: Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,651 INFO cluster.YarnScheduler: Killing all running tasks in stage 4: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,651 INFO scheduler.DAGScheduler: Job 3 finished: collect at AnalysisRunner.scala:326, took 0.449907 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:08,712 INFO codegen.CodeGenerator: Code generated in 49.5563 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,010 INFO codegen.CodeGenerator: Code generated in 33.29392 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,099 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,101 INFO scheduler.DAGScheduler: Got job 4 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,102 INFO scheduler.DAGScheduler: Final stage: ResultStage 5 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,102 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,103 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,104 INFO scheduler.DAGScheduler: Submitting ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,118 INFO memory.MemoryStore: Block broadcast_6 stored as values in memory (estimated size 40.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,120 INFO memory.MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 17.2 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,121 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on 10.0.79.62:35505 (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,122 INFO spark.SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,123 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 5 (MapPartitionsRDD[29] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,123 INFO cluster.YarnScheduler: Adding task set 5.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,125 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 5.0 (TID 4) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:09,139 INFO storage.BlockManagerInfo: Added broadcast_6_piece0 in memory on algo-1:41655 (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,042 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 5.0 (TID 4) in 1917 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,043 INFO scheduler.DAGScheduler: ResultStage 5 (treeReduce at KLLRunner.scala:107) finished in 1.938 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,044 INFO scheduler.DAGScheduler: Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,044 INFO cluster.YarnScheduler: Removed TaskSet 5.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,044 INFO cluster.YarnScheduler: Killing all running tasks in stage 5: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,045 INFO scheduler.DAGScheduler: Job 4 finished: treeReduce at KLLRunner.scala:107, took 1.944840 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,557 INFO codegen.CodeGenerator: Code generated in 101.278364 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,566 INFO scheduler.DAGScheduler: Registering RDD 34 (collect at AnalysisRunner.scala:326) as input to shuffle 1\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,566 INFO scheduler.DAGScheduler: Got map stage job 5 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,566 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,566 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,567 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,568 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,572 INFO memory.MemoryStore: Block broadcast_7 stored as values in memory (estimated size 77.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,575 INFO memory.MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 24.9 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,575 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on 10.0.79.62:35505 (size: 24.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,576 INFO spark.SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,577 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 6 (MapPartitionsRDD[34] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,577 INFO cluster.YarnScheduler: Adding task set 6.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,578 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 6.0 (TID 5) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,596 INFO storage.BlockManagerInfo: Added broadcast_7_piece0 in memory on algo-1:41655 (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,799 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 6.0 (TID 5) in 221 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,799 INFO cluster.YarnScheduler: Removed TaskSet 6.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,800 INFO scheduler.DAGScheduler: ShuffleMapStage 6 (collect at AnalysisRunner.scala:326) finished in 0.231 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,800 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,800 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,800 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,800 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,976 INFO codegen.CodeGenerator: Code generated in 97.375699 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,989 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,991 INFO scheduler.DAGScheduler: Got job 6 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,991 INFO scheduler.DAGScheduler: Final stage: ResultStage 8 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,991 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 7)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,991 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,992 INFO scheduler.DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,995 INFO memory.MemoryStore: Block broadcast_8 stored as values in memory (estimated size 67.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,997 INFO memory.MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 19.7 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,998 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on 10.0.79.62:35505 (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,999 INFO spark.SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,999 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[37] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:11,999 INFO cluster.YarnScheduler: Adding task set 8.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,001 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 8.0 (TID 6) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,015 INFO storage.BlockManagerInfo: Added broadcast_8_piece0 in memory on algo-1:41655 (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,021 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 1 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,137 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 8.0 (TID 6) in 136 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,137 INFO cluster.YarnScheduler: Removed TaskSet 8.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,139 INFO scheduler.DAGScheduler: ResultStage 8 (collect at AnalysisRunner.scala:326) finished in 0.145 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,140 INFO scheduler.DAGScheduler: Job 6 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,141 INFO cluster.YarnScheduler: Killing all running tasks in stage 8: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,142 INFO scheduler.DAGScheduler: Job 6 finished: collect at AnalysisRunner.scala:326, took 0.151919 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,206 INFO codegen.CodeGenerator: Code generated in 44.15933 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,329 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,333 INFO scheduler.DAGScheduler: Registering RDD 45 (countByKey at ColumnProfiler.scala:592) as input to shuffle 2\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,333 INFO scheduler.DAGScheduler: Got job 7 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,334 INFO scheduler.DAGScheduler: Final stage: ResultStage 10 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,334 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,334 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 9)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,338 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,353 INFO memory.MemoryStore: Block broadcast_9 stored as values in memory (estimated size 32.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,355 INFO memory.MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 14.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,356 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on 10.0.79.62:35505 (size: 14.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,356 INFO spark.SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,361 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[45] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,362 INFO cluster.YarnScheduler: Adding task set 9.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,364 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 9.0 (TID 7) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:12,382 INFO storage.BlockManagerInfo: Added broadcast_9_piece0 in memory on algo-1:41655 (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,005 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 9.0 (TID 7) in 1642 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,005 INFO cluster.YarnScheduler: Removed TaskSet 9.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,006 INFO scheduler.DAGScheduler: ShuffleMapStage 9 (countByKey at ColumnProfiler.scala:592) finished in 1.667 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,007 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,007 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,007 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 10)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,007 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,008 INFO scheduler.DAGScheduler: Submitting ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,013 INFO memory.MemoryStore: Block broadcast_10 stored as values in memory (estimated size 5.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,015 INFO memory.MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,016 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on 10.0.79.62:35505 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,016 INFO spark.SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,017 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (ShuffledRDD[46] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,017 INFO cluster.YarnScheduler: Adding task set 10.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,019 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 10.0 (TID 8) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,033 INFO storage.BlockManagerInfo: Added broadcast_10_piece0 in memory on algo-1:41655 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,041 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 2 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,097 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 10.0 (TID 8) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,097 INFO cluster.YarnScheduler: Removed TaskSet 10.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,098 INFO scheduler.DAGScheduler: ResultStage 10 (countByKey at ColumnProfiler.scala:592) finished in 0.089 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,098 INFO scheduler.DAGScheduler: Job 7 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,098 INFO cluster.YarnScheduler: Killing all running tasks in stage 10: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,099 INFO scheduler.DAGScheduler: Job 7 finished: countByKey at ColumnProfiler.scala:592, took 1.769551 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,240 INFO scheduler.DAGScheduler: Registering RDD 51 (collect at AnalysisRunner.scala:326) as input to shuffle 3\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,241 INFO scheduler.DAGScheduler: Got map stage job 8 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,241 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,241 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,242 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,244 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,250 INFO memory.MemoryStore: Block broadcast_11 stored as values in memory (estimated size 85.6 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,252 INFO memory.MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 28.2 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,253 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on 10.0.79.62:35505 (size: 28.2 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,254 INFO spark.SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,254 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[51] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,254 INFO cluster.YarnScheduler: Adding task set 11.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,256 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 11.0 (TID 9) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,269 INFO storage.BlockManagerInfo: Added broadcast_11_piece0 in memory on algo-1:41655 (size: 28.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,651 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 11.0 (TID 9) in 395 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,651 INFO cluster.YarnScheduler: Removed TaskSet 11.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,652 INFO scheduler.DAGScheduler: ShuffleMapStage 11 (collect at AnalysisRunner.scala:326) finished in 0.407 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,653 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,653 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,653 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,653 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,693 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,694 INFO scheduler.DAGScheduler: Got job 9 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,694 INFO scheduler.DAGScheduler: Final stage: ResultStage 13 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,694 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 12)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,695 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,696 INFO scheduler.DAGScheduler: Submitting ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,705 INFO memory.MemoryStore: Block broadcast_12 stored as values in memory (estimated size 170.4 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,710 INFO memory.MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,711 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on 10.0.79.62:35505 (size: 47.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,711 INFO spark.SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,711 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 13 (MapPartitionsRDD[54] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,711 INFO cluster.YarnScheduler: Adding task set 13.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,713 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 13.0 (TID 10) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,729 INFO storage.BlockManagerInfo: Added broadcast_12_piece0 in memory on algo-1:41655 (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,740 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 3 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,856 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 13.0 (TID 10) in 143 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,856 INFO cluster.YarnScheduler: Removed TaskSet 13.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,857 INFO scheduler.DAGScheduler: ResultStage 13 (collect at AnalysisRunner.scala:326) finished in 0.159 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,858 INFO scheduler.DAGScheduler: Job 9 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,858 INFO cluster.YarnScheduler: Killing all running tasks in stage 13: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,859 INFO scheduler.DAGScheduler: Job 9 finished: collect at AnalysisRunner.scala:326, took 0.165782 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:14,974 INFO codegen.CodeGenerator: Code generated in 14.067834 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,027 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on algo-1:41655 in memory (size: 35.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,035 INFO storage.BlockManagerInfo: Removed broadcast_4_piece0 on 10.0.79.62:35505 in memory (size: 35.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,040 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,041 INFO scheduler.DAGScheduler: Got job 10 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,041 INFO scheduler.DAGScheduler: Final stage: ResultStage 14 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,041 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,042 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,043 INFO scheduler.DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,052 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on algo-1:41655 in memory (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,054 INFO memory.MemoryStore: Block broadcast_13 stored as values in memory (estimated size 39.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,055 INFO storage.BlockManagerInfo: Removed broadcast_12_piece0 on 10.0.79.62:35505 in memory (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,056 INFO memory.MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,056 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on 10.0.79.62:35505 (size: 16.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,057 INFO spark.SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,057 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[64] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,057 INFO cluster.YarnScheduler: Adding task set 14.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,059 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 14.0 (TID 11) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,060 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on 10.0.79.62:35505 in memory (size: 17.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,062 INFO storage.BlockManagerInfo: Removed broadcast_6_piece0 on algo-1:41655 in memory (size: 17.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,069 INFO storage.BlockManagerInfo: Added broadcast_13_piece0 in memory on algo-1:41655 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,072 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on 10.0.79.62:35505 in memory (size: 46.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,075 INFO storage.BlockManagerInfo: Removed broadcast_5_piece0 on algo-1:41655 in memory (size: 46.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,087 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on 10.0.79.62:35505 in memory (size: 19.7 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,092 INFO storage.BlockManagerInfo: Removed broadcast_8_piece0 on algo-1:41655 in memory (size: 19.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,121 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on 10.0.79.62:35505 in memory (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,133 INFO storage.BlockManagerInfo: Removed broadcast_10_piece0 on algo-1:41655 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,197 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on algo-1:41655 in memory (size: 14.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,198 INFO storage.BlockManagerInfo: Removed broadcast_9_piece0 on 10.0.79.62:35505 in memory (size: 14.8 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,229 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on 10.0.79.62:35505 in memory (size: 28.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,236 INFO storage.BlockManagerInfo: Removed broadcast_11_piece0 on algo-1:41655 in memory (size: 28.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,260 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on 10.0.79.62:35505 in memory (size: 24.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:15,263 INFO storage.BlockManagerInfo: Removed broadcast_7_piece0 on algo-1:41655 in memory (size: 24.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,415 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 14.0 (TID 11) in 1356 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,415 INFO cluster.YarnScheduler: Removed TaskSet 14.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,416 INFO scheduler.DAGScheduler: ResultStage 14 (treeReduce at KLLRunner.scala:107) finished in 1.371 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,416 INFO scheduler.DAGScheduler: Job 10 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,417 INFO cluster.YarnScheduler: Killing all running tasks in stage 14: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,417 INFO scheduler.DAGScheduler: Job 10 finished: treeReduce at KLLRunner.scala:107, took 1.377000 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,637 INFO codegen.CodeGenerator: Code generated in 46.355669 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,644 INFO scheduler.DAGScheduler: Registering RDD 69 (collect at AnalysisRunner.scala:326) as input to shuffle 4\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,645 INFO scheduler.DAGScheduler: Got map stage job 11 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,645 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,646 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,646 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,647 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,650 INFO memory.MemoryStore: Block broadcast_14 stored as values in memory (estimated size 78.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,653 INFO memory.MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 25.2 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,653 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on 10.0.79.62:35505 (size: 25.2 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,654 INFO spark.SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,654 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 15 (MapPartitionsRDD[69] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,654 INFO cluster.YarnScheduler: Adding task set 15.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,656 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 15.0 (TID 12) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,668 INFO storage.BlockManagerInfo: Added broadcast_14_piece0 in memory on algo-1:41655 (size: 25.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,839 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 15.0 (TID 12) in 184 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,839 INFO cluster.YarnScheduler: Removed TaskSet 15.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,840 INFO scheduler.DAGScheduler: ShuffleMapStage 15 (collect at AnalysisRunner.scala:326) finished in 0.192 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,841 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,841 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,841 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,841 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:16,989 INFO codegen.CodeGenerator: Code generated in 93.709555 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,002 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,004 INFO scheduler.DAGScheduler: Got job 12 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,004 INFO scheduler.DAGScheduler: Final stage: ResultStage 17 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,004 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,004 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,005 INFO scheduler.DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,007 INFO memory.MemoryStore: Block broadcast_15 stored as values in memory (estimated size 67.7 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,009 INFO memory.MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,010 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on 10.0.79.62:35505 (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,011 INFO spark.SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,011 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[72] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,011 INFO cluster.YarnScheduler: Adding task set 17.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,013 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 17.0 (TID 13) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,024 INFO storage.BlockManagerInfo: Added broadcast_15_piece0 in memory on algo-1:41655 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,029 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 4 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,097 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 17.0 (TID 13) in 84 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,097 INFO cluster.YarnScheduler: Removed TaskSet 17.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,097 INFO scheduler.DAGScheduler: ResultStage 17 (collect at AnalysisRunner.scala:326) finished in 0.091 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,098 INFO scheduler.DAGScheduler: Job 12 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,098 INFO cluster.YarnScheduler: Killing all running tasks in stage 17: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,100 INFO scheduler.DAGScheduler: Job 12 finished: collect at AnalysisRunner.scala:326, took 0.097202 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,216 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,218 INFO scheduler.DAGScheduler: Registering RDD 80 (countByKey at ColumnProfiler.scala:592) as input to shuffle 5\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,219 INFO scheduler.DAGScheduler: Got job 13 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,219 INFO scheduler.DAGScheduler: Final stage: ResultStage 19 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,219 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,220 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 18)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,221 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,226 INFO memory.MemoryStore: Block broadcast_16 stored as values in memory (estimated size 32.8 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,229 INFO memory.MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,230 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on 10.0.79.62:35505 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,231 INFO spark.SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,232 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[80] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,232 INFO cluster.YarnScheduler: Adding task set 18.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,234 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 18.0 (TID 14) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,249 INFO storage.BlockManagerInfo: Added broadcast_16_piece0 in memory on algo-1:41655 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,507 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 18.0 (TID 14) in 274 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,507 INFO cluster.YarnScheduler: Removed TaskSet 18.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,508 INFO scheduler.DAGScheduler: ShuffleMapStage 18 (countByKey at ColumnProfiler.scala:592) finished in 0.286 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,509 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,509 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,509 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 19)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,509 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,510 INFO scheduler.DAGScheduler: Submitting ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,512 INFO memory.MemoryStore: Block broadcast_17 stored as values in memory (estimated size 5.1 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,514 INFO memory.MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,514 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on 10.0.79.62:35505 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,515 INFO spark.SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,516 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (ShuffledRDD[81] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,516 INFO cluster.YarnScheduler: Adding task set 19.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,518 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 19.0 (TID 15) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,530 INFO storage.BlockManagerInfo: Added broadcast_17_piece0 in memory on algo-1:41655 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,533 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 5 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,554 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 19.0 (TID 15) in 37 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,554 INFO cluster.YarnScheduler: Removed TaskSet 19.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,554 INFO scheduler.DAGScheduler: ResultStage 19 (countByKey at ColumnProfiler.scala:592) finished in 0.043 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,555 INFO scheduler.DAGScheduler: Job 13 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,555 INFO cluster.YarnScheduler: Killing all running tasks in stage 19: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,555 INFO scheduler.DAGScheduler: Job 13 finished: countByKey at ColumnProfiler.scala:592, took 0.338424 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,710 INFO scheduler.DAGScheduler: Registering RDD 86 (collect at AnalysisRunner.scala:326) as input to shuffle 6\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,710 INFO scheduler.DAGScheduler: Got map stage job 14 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,710 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,710 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,711 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,711 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,714 INFO memory.MemoryStore: Block broadcast_18 stored as values in memory (estimated size 85.6 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,716 INFO memory.MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,717 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on 10.0.79.62:35505 (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,717 INFO spark.SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,718 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[86] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,718 INFO cluster.YarnScheduler: Adding task set 20.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,719 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 20.0 (TID 16) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:17,730 INFO storage.BlockManagerInfo: Added broadcast_18_piece0 in memory on algo-1:41655 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,150 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 20.0 (TID 16) in 431 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,150 INFO cluster.YarnScheduler: Removed TaskSet 20.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,151 INFO scheduler.DAGScheduler: ShuffleMapStage 20 (collect at AnalysisRunner.scala:326) finished in 0.439 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,151 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,151 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,151 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,151 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,188 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,189 INFO scheduler.DAGScheduler: Got job 15 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,189 INFO scheduler.DAGScheduler: Final stage: ResultStage 22 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,189 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 21)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,189 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,190 INFO scheduler.DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,198 INFO memory.MemoryStore: Block broadcast_19 stored as values in memory (estimated size 170.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,200 INFO memory.MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 46.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,201 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on 10.0.79.62:35505 (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,202 INFO spark.SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,202 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[89] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,202 INFO cluster.YarnScheduler: Adding task set 22.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,204 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 22.0 (TID 17) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,215 INFO storage.BlockManagerInfo: Added broadcast_19_piece0 in memory on algo-1:41655 (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,226 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 6 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,385 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 22.0 (TID 17) in 181 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,385 INFO cluster.YarnScheduler: Removed TaskSet 22.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,386 INFO scheduler.DAGScheduler: ResultStage 22 (collect at AnalysisRunner.scala:326) finished in 0.194 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,397 INFO scheduler.DAGScheduler: Job 15 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,397 INFO cluster.YarnScheduler: Killing all running tasks in stage 22: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,398 INFO scheduler.DAGScheduler: Job 15 finished: collect at AnalysisRunner.scala:326, took 0.209625 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,494 INFO codegen.CodeGenerator: Code generated in 23.728411 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,540 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,541 INFO scheduler.DAGScheduler: Got job 16 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,542 INFO scheduler.DAGScheduler: Final stage: ResultStage 23 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,542 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,543 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,544 INFO scheduler.DAGScheduler: Submitting ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,550 INFO memory.MemoryStore: Block broadcast_20 stored as values in memory (estimated size 39.5 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,556 INFO memory.MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 17.5 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,557 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on 10.0.79.62:35505 (size: 17.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,558 INFO spark.SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,558 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 23 (MapPartitionsRDD[99] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,559 INFO cluster.YarnScheduler: Adding task set 23.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,561 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 23.0 (TID 18) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:18,577 INFO storage.BlockManagerInfo: Added broadcast_20_piece0 in memory on algo-1:41655 (size: 17.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,337 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 23.0 (TID 18) in 777 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,337 INFO cluster.YarnScheduler: Removed TaskSet 23.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,338 INFO scheduler.DAGScheduler: ResultStage 23 (treeReduce at KLLRunner.scala:107) finished in 0.793 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,338 INFO scheduler.DAGScheduler: Job 16 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,338 INFO cluster.YarnScheduler: Killing all running tasks in stage 23: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,338 INFO scheduler.DAGScheduler: Job 16 finished: treeReduce at KLLRunner.scala:107, took 0.798342 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,465 INFO codegen.CodeGenerator: Code generated in 37.983172 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,471 INFO scheduler.DAGScheduler: Registering RDD 104 (collect at AnalysisRunner.scala:326) as input to shuffle 7\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,472 INFO scheduler.DAGScheduler: Got map stage job 17 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,472 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,472 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,472 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,473 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,476 INFO memory.MemoryStore: Block broadcast_21 stored as values in memory (estimated size 56.2 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,478 INFO memory.MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 19.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,478 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on 10.0.79.62:35505 (size: 19.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,478 INFO spark.SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,481 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 24 (MapPartitionsRDD[104] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,481 INFO cluster.YarnScheduler: Adding task set 24.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,483 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 24.0 (TID 19) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,492 INFO storage.BlockManagerInfo: Added broadcast_21_piece0 in memory on algo-1:41655 (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,743 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 24.0 (TID 19) in 261 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,743 INFO cluster.YarnScheduler: Removed TaskSet 24.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,743 INFO scheduler.DAGScheduler: ShuffleMapStage 24 (collect at AnalysisRunner.scala:326) finished in 0.270 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,748 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,749 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,749 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,749 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,842 INFO codegen.CodeGenerator: Code generated in 36.419203 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,853 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,855 INFO scheduler.DAGScheduler: Got job 18 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,855 INFO scheduler.DAGScheduler: Final stage: ResultStage 26 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,855 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,855 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,856 INFO scheduler.DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,858 INFO memory.MemoryStore: Block broadcast_22 stored as values in memory (estimated size 44.4 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,863 INFO memory.MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 14.3 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,863 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on 10.0.79.62:35505 (size: 14.3 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,864 INFO spark.SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,864 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[107] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,865 INFO cluster.YarnScheduler: Adding task set 26.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,866 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 26.0 (TID 20) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,904 INFO storage.BlockManagerInfo: Added broadcast_22_piece0 in memory on algo-1:41655 (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,907 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 7 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,965 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 26.0 (TID 20) in 99 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,965 INFO cluster.YarnScheduler: Removed TaskSet 26.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,966 INFO scheduler.DAGScheduler: ResultStage 26 (collect at AnalysisRunner.scala:326) finished in 0.109 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,966 INFO scheduler.DAGScheduler: Job 18 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,966 INFO cluster.YarnScheduler: Killing all running tasks in stage 26: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:19,967 INFO scheduler.DAGScheduler: Job 18 finished: collect at AnalysisRunner.scala:326, took 0.112811 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,009 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on 10.0.79.62:35505 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,020 INFO storage.BlockManagerInfo: Removed broadcast_17_piece0 on algo-1:41655 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,033 INFO codegen.CodeGenerator: Code generated in 41.830245 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,071 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on 10.0.79.62:35505 in memory (size: 46.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,078 INFO storage.BlockManagerInfo: Removed broadcast_19_piece0 on algo-1:41655 in memory (size: 46.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,081 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,082 INFO scheduler.DAGScheduler: Registering RDD 115 (countByKey at ColumnProfiler.scala:592) as input to shuffle 8\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,082 INFO scheduler.DAGScheduler: Got job 19 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,082 INFO scheduler.DAGScheduler: Final stage: ResultStage 28 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,082 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,082 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 27)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,083 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,103 INFO memory.MemoryStore: Block broadcast_23 stored as values in memory (estimated size 32.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,105 INFO memory.MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,105 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on algo-1:41655 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,107 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on 10.0.79.62:35505 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,107 INFO storage.BlockManagerInfo: Removed broadcast_18_piece0 on 10.0.79.62:35505 in memory (size: 28.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,111 INFO spark.SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,111 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 27 (MapPartitionsRDD[115] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,111 INFO cluster.YarnScheduler: Adding task set 27.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,112 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 27.0 (TID 21) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,117 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on 10.0.79.62:35505 in memory (size: 14.3 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,126 INFO storage.BlockManagerInfo: Removed broadcast_22_piece0 on algo-1:41655 in memory (size: 14.3 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,129 INFO storage.BlockManagerInfo: Added broadcast_23_piece0 in memory on algo-1:41655 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,143 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on algo-1:41655 in memory (size: 25.2 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,153 INFO storage.BlockManagerInfo: Removed broadcast_14_piece0 on 10.0.79.62:35505 in memory (size: 25.2 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,157 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on 10.0.79.62:35505 in memory (size: 17.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,166 INFO storage.BlockManagerInfo: Removed broadcast_20_piece0 on algo-1:41655 in memory (size: 17.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,178 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on 10.0.79.62:35505 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,189 INFO storage.BlockManagerInfo: Removed broadcast_15_piece0 on algo-1:41655 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,202 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on 10.0.79.62:35505 in memory (size: 14.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,204 INFO storage.BlockManagerInfo: Removed broadcast_16_piece0 on algo-1:41655 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,209 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on algo-1:41655 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,210 INFO storage.BlockManagerInfo: Removed broadcast_13_piece0 on 10.0.79.62:35505 in memory (size: 16.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,213 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on 10.0.79.62:35505 in memory (size: 19.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,215 INFO storage.BlockManagerInfo: Removed broadcast_21_piece0 on algo-1:41655 in memory (size: 19.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,263 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 27.0 (TID 21) in 151 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,263 INFO cluster.YarnScheduler: Removed TaskSet 27.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: ShuffleMapStage 27 (countByKey at ColumnProfiler.scala:592) finished in 0.180 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 28)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,264 INFO scheduler.DAGScheduler: Submitting ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,266 INFO memory.MemoryStore: Block broadcast_24 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,267 INFO memory.MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,268 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on 10.0.79.62:35505 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,269 INFO spark.SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,269 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 28 (ShuffledRDD[116] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,269 INFO cluster.YarnScheduler: Adding task set 28.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,271 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 28.0 (TID 22) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,281 INFO storage.BlockManagerInfo: Added broadcast_24_piece0 in memory on algo-1:41655 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,285 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 8 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,295 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 28.0 (TID 22) in 24 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,295 INFO cluster.YarnScheduler: Removed TaskSet 28.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,296 INFO scheduler.DAGScheduler: ResultStage 28 (countByKey at ColumnProfiler.scala:592) finished in 0.031 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,296 INFO scheduler.DAGScheduler: Job 19 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,296 INFO cluster.YarnScheduler: Killing all running tasks in stage 28: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,296 INFO scheduler.DAGScheduler: Job 19 finished: countByKey at ColumnProfiler.scala:592, took 0.215052 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,438 INFO scheduler.DAGScheduler: Registering RDD 121 (collect at AnalysisRunner.scala:326) as input to shuffle 9\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,438 INFO scheduler.DAGScheduler: Got map stage job 20 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,438 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,439 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,439 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,440 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,444 INFO memory.MemoryStore: Block broadcast_25 stored as values in memory (estimated size 85.6 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,446 INFO memory.MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 28.0 KiB, free 1457.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,447 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on 10.0.79.62:35505 (size: 28.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,447 INFO spark.SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,447 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 29 (MapPartitionsRDD[121] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,447 INFO cluster.YarnScheduler: Adding task set 29.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,449 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 29.0 (TID 23) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,461 INFO storage.BlockManagerInfo: Added broadcast_25_piece0 in memory on algo-1:41655 (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,807 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 29.0 (TID 23) in 358 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO cluster.YarnScheduler: Removed TaskSet 29.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO scheduler.DAGScheduler: ShuffleMapStage 29 (collect at AnalysisRunner.scala:326) finished in 0.367 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,808 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,838 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,839 INFO scheduler.DAGScheduler: Got job 21 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,839 INFO scheduler.DAGScheduler: Final stage: ResultStage 31 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,839 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,839 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,839 INFO scheduler.DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,844 INFO memory.MemoryStore: Block broadcast_26 stored as values in memory (estimated size 170.6 KiB, free 1457.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,846 INFO memory.MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 47.0 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,847 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on 10.0.79.62:35505 (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,847 INFO spark.SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,848 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[124] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,848 INFO cluster.YarnScheduler: Adding task set 31.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,849 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 31.0 (TID 24) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,857 INFO storage.BlockManagerInfo: Added broadcast_26_piece0 in memory on algo-1:41655 (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,864 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 9 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,928 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 31.0 (TID 24) in 79 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,930 INFO cluster.YarnScheduler: Removed TaskSet 31.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,930 INFO scheduler.DAGScheduler: ResultStage 31 (collect at AnalysisRunner.scala:326) finished in 0.090 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,934 INFO scheduler.DAGScheduler: Job 21 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,934 INFO cluster.YarnScheduler: Killing all running tasks in stage 31: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:20,934 INFO scheduler.DAGScheduler: Job 21 finished: collect at AnalysisRunner.scala:326, took 0.096135 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,033 INFO codegen.CodeGenerator: Code generated in 21.155545 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,065 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,066 INFO scheduler.DAGScheduler: Got job 22 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,066 INFO scheduler.DAGScheduler: Final stage: ResultStage 32 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,066 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,066 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,067 INFO scheduler.DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,074 INFO memory.MemoryStore: Block broadcast_27 stored as values in memory (estimated size 40.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,076 INFO memory.MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 17.1 KiB, free 1457.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,076 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on 10.0.79.62:35505 (size: 17.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,077 INFO spark.SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,077 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[134] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,077 INFO cluster.YarnScheduler: Adding task set 32.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,079 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 32.0 (TID 25) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:21,097 INFO storage.BlockManagerInfo: Added broadcast_27_piece0 in memory on algo-1:41655 (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,347 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 32.0 (TID 25) in 1269 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,347 INFO cluster.YarnScheduler: Removed TaskSet 32.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,347 INFO scheduler.DAGScheduler: ResultStage 32 (treeReduce at KLLRunner.scala:107) finished in 1.279 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,348 INFO scheduler.DAGScheduler: Job 22 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,348 INFO cluster.YarnScheduler: Killing all running tasks in stage 32: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,348 INFO scheduler.DAGScheduler: Job 22 finished: treeReduce at KLLRunner.scala:107, took 1.283196 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,537 INFO codegen.CodeGenerator: Code generated in 40.401404 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,543 INFO scheduler.DAGScheduler: Registering RDD 139 (collect at AnalysisRunner.scala:326) as input to shuffle 10\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,543 INFO scheduler.DAGScheduler: Got map stage job 23 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,543 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,543 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,544 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,544 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,549 INFO memory.MemoryStore: Block broadcast_28 stored as values in memory (estimated size 77.3 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,551 INFO memory.MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 25.0 KiB, free 1457.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,551 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on 10.0.79.62:35505 (size: 25.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,552 INFO spark.SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,552 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 33 (MapPartitionsRDD[139] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,552 INFO cluster.YarnScheduler: Adding task set 33.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,554 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 33.0 (TID 26) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,565 INFO storage.BlockManagerInfo: Added broadcast_28_piece0 in memory on algo-1:41655 (size: 25.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,819 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 33.0 (TID 26) in 265 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,821 INFO cluster.YarnScheduler: Removed TaskSet 33.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,821 INFO scheduler.DAGScheduler: ShuffleMapStage 33 (collect at AnalysisRunner.scala:326) finished in 0.275 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,822 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,822 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,823 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,823 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,956 INFO codegen.CodeGenerator: Code generated in 85.830699 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,982 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,984 INFO scheduler.DAGScheduler: Got job 24 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,984 INFO scheduler.DAGScheduler: Final stage: ResultStage 35 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,984 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 34)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,985 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,985 INFO scheduler.DAGScheduler: Submitting ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,987 INFO memory.MemoryStore: Block broadcast_29 stored as values in memory (estimated size 67.1 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,989 INFO memory.MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 19.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,991 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on 10.0.79.62:35505 (size: 19.8 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,992 INFO spark.SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,992 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 35 (MapPartitionsRDD[142] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,992 INFO cluster.YarnScheduler: Adding task set 35.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:22,994 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 35.0 (TID 27) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,005 INFO storage.BlockManagerInfo: Added broadcast_29_piece0 in memory on algo-1:41655 (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,010 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 10 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,082 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 35.0 (TID 27) in 89 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,083 INFO cluster.YarnScheduler: Removed TaskSet 35.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,083 INFO scheduler.DAGScheduler: ResultStage 35 (collect at AnalysisRunner.scala:326) finished in 0.097 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,084 INFO scheduler.DAGScheduler: Job 24 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,084 INFO cluster.YarnScheduler: Killing all running tasks in stage 35: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,088 INFO scheduler.DAGScheduler: Job 24 finished: collect at AnalysisRunner.scala:326, took 0.105356 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,215 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,217 INFO scheduler.DAGScheduler: Registering RDD 150 (countByKey at ColumnProfiler.scala:592) as input to shuffle 11\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,217 INFO scheduler.DAGScheduler: Got job 25 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,217 INFO scheduler.DAGScheduler: Final stage: ResultStage 37 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,217 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,217 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 36)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,218 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,227 INFO memory.MemoryStore: Block broadcast_30 stored as values in memory (estimated size 32.8 KiB, free 1457.1 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,229 INFO memory.MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,230 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on 10.0.79.62:35505 (size: 14.9 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,231 INFO spark.SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,231 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 36 (MapPartitionsRDD[150] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,231 INFO cluster.YarnScheduler: Adding task set 36.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,234 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 36.0 (TID 28) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,248 INFO storage.BlockManagerInfo: Added broadcast_30_piece0 in memory on algo-1:41655 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,403 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 36.0 (TID 28) in 170 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,403 INFO cluster.YarnScheduler: Removed TaskSet 36.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: ShuffleMapStage 36 (countByKey at ColumnProfiler.scala:592) finished in 0.184 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 37)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,404 INFO scheduler.DAGScheduler: Submitting ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,406 INFO memory.MemoryStore: Block broadcast_31 stored as values in memory (estimated size 5.1 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,407 INFO memory.MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,409 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on 10.0.79.62:35505 (size: 3.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,409 INFO spark.SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,409 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 37 (ShuffledRDD[151] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,409 INFO cluster.YarnScheduler: Adding task set 37.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,410 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 37.0 (TID 29) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,418 INFO storage.BlockManagerInfo: Added broadcast_31_piece0 in memory on algo-1:41655 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,422 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 11 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,441 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 37.0 (TID 29) in 31 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,441 INFO cluster.YarnScheduler: Removed TaskSet 37.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,442 INFO scheduler.DAGScheduler: ResultStage 37 (countByKey at ColumnProfiler.scala:592) finished in 0.037 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,442 INFO scheduler.DAGScheduler: Job 25 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,442 INFO cluster.YarnScheduler: Killing all running tasks in stage 37: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,443 INFO scheduler.DAGScheduler: Job 25 finished: countByKey at ColumnProfiler.scala:592, took 0.227675 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,509 INFO scheduler.DAGScheduler: Registering RDD 156 (collect at AnalysisRunner.scala:326) as input to shuffle 12\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,509 INFO scheduler.DAGScheduler: Got map stage job 26 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,510 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,510 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,510 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,510 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,514 INFO memory.MemoryStore: Block broadcast_32 stored as values in memory (estimated size 75.4 KiB, free 1457.0 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,515 INFO memory.MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 25.7 KiB, free 1456.9 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,515 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on 10.0.79.62:35505 (size: 25.7 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,516 INFO spark.SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,516 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 38 (MapPartitionsRDD[156] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,516 INFO cluster.YarnScheduler: Adding task set 38.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,517 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 38.0 (TID 30) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:23,528 INFO storage.BlockManagerInfo: Added broadcast_32_piece0 in memory on algo-1:41655 (size: 25.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,089 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 38.0 (TID 30) in 572 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,089 INFO cluster.YarnScheduler: Removed TaskSet 38.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,090 INFO scheduler.DAGScheduler: ShuffleMapStage 38 (collect at AnalysisRunner.scala:326) finished in 0.578 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,090 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,090 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,091 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,091 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,122 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,123 INFO scheduler.DAGScheduler: Got job 27 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,123 INFO scheduler.DAGScheduler: Final stage: ResultStage 40 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,123 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 39)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,123 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,124 INFO scheduler.DAGScheduler: Submitting ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,130 INFO memory.MemoryStore: Block broadcast_33 stored as values in memory (estimated size 144.2 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,134 INFO memory.MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 41.1 KiB, free 1456.8 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,135 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on 10.0.79.62:35505 (size: 41.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,137 INFO spark.SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,138 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 40 (MapPartitionsRDD[159] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,138 INFO cluster.YarnScheduler: Adding task set 40.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,139 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 40.0 (TID 31) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,152 INFO storage.BlockManagerInfo: Added broadcast_33_piece0 in memory on algo-1:41655 (size: 41.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,160 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 12 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,261 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 40.0 (TID 31) in 122 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,262 INFO scheduler.DAGScheduler: ResultStage 40 (collect at AnalysisRunner.scala:326) finished in 0.137 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,262 INFO scheduler.DAGScheduler: Job 27 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,263 INFO cluster.YarnScheduler: Removed TaskSet 40.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,263 INFO cluster.YarnScheduler: Killing all running tasks in stage 40: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,264 INFO scheduler.DAGScheduler: Job 27 finished: collect at AnalysisRunner.scala:326, took 0.141354 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,276 INFO codegen.CodeGenerator: Code generated in 9.509674 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,363 INFO codegen.CodeGenerator: Code generated in 19.405974 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,393 INFO spark.SparkContext: Starting job: treeReduce at KLLRunner.scala:107\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,393 INFO scheduler.DAGScheduler: Got job 28 (treeReduce at KLLRunner.scala:107) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,394 INFO scheduler.DAGScheduler: Final stage: ResultStage 41 (treeReduce at KLLRunner.scala:107)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,394 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,394 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,395 INFO scheduler.DAGScheduler: Submitting ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,402 INFO memory.MemoryStore: Block broadcast_34 stored as values in memory (estimated size 38.3 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,404 INFO memory.MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 16.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,404 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on 10.0.79.62:35505 (size: 16.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,405 INFO spark.SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,405 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 41 (MapPartitionsRDD[169] at treeReduce at KLLRunner.scala:107) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,405 INFO cluster.YarnScheduler: Adding task set 41.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,407 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 41.0 (TID 32) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4964 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:24,417 INFO storage.BlockManagerInfo: Added broadcast_34_piece0 in memory on algo-1:41655 (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,064 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 41.0 (TID 32) in 657 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,064 INFO cluster.YarnScheduler: Removed TaskSet 41.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,065 INFO scheduler.DAGScheduler: ResultStage 41 (treeReduce at KLLRunner.scala:107) finished in 0.669 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,066 INFO scheduler.DAGScheduler: Job 28 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,066 INFO cluster.YarnScheduler: Killing all running tasks in stage 41: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,066 INFO scheduler.DAGScheduler: Job 28 finished: treeReduce at KLLRunner.scala:107, took 0.673507 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,226 INFO codegen.CodeGenerator: Code generated in 33.555896 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,239 INFO scheduler.DAGScheduler: Registering RDD 174 (collect at AnalysisRunner.scala:326) as input to shuffle 13\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,240 INFO scheduler.DAGScheduler: Got map stage job 29 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,240 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,240 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,241 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,241 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,250 INFO memory.MemoryStore: Block broadcast_35 stored as values in memory (estimated size 45.9 KiB, free 1456.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,254 INFO memory.MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 17.4 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,255 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on 10.0.79.62:35505 (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,256 INFO spark.SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,256 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 42 (MapPartitionsRDD[174] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,258 INFO cluster.YarnScheduler: Adding task set 42.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,259 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 42.0 (TID 33) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,272 INFO storage.BlockManagerInfo: Added broadcast_35_piece0 in memory on algo-1:41655 (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,452 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 42.0 (TID 33) in 193 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,452 INFO cluster.YarnScheduler: Removed TaskSet 42.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,453 INFO scheduler.DAGScheduler: ShuffleMapStage 42 (collect at AnalysisRunner.scala:326) finished in 0.210 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,454 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,454 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,454 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,454 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,518 INFO codegen.CodeGenerator: Code generated in 28.501874 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,538 INFO spark.SparkContext: Starting job: collect at AnalysisRunner.scala:326\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,540 INFO scheduler.DAGScheduler: Got job 30 (collect at AnalysisRunner.scala:326) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,540 INFO scheduler.DAGScheduler: Final stage: ResultStage 44 (collect at AnalysisRunner.scala:326)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,540 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 43)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,540 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,541 INFO scheduler.DAGScheduler: Submitting ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,543 INFO memory.MemoryStore: Block broadcast_36 stored as values in memory (estimated size 33.2 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,545 INFO memory.MemoryStore: Block broadcast_36_piece0 stored as bytes in memory (estimated size 11.5 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,545 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on 10.0.79.62:35505 (size: 11.5 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,546 INFO spark.SparkContext: Created broadcast 36 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,546 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 44 (MapPartitionsRDD[177] at collect at AnalysisRunner.scala:326) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,547 INFO cluster.YarnScheduler: Adding task set 44.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,548 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 44.0 (TID 34) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,591 INFO storage.BlockManagerInfo: Added broadcast_36_piece0 in memory on algo-1:41655 (size: 11.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,595 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 13 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,619 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 44.0 (TID 34) in 70 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,619 INFO cluster.YarnScheduler: Removed TaskSet 44.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,621 INFO scheduler.DAGScheduler: ResultStage 44 (collect at AnalysisRunner.scala:326) finished in 0.079 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,621 INFO scheduler.DAGScheduler: Job 30 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,622 INFO cluster.YarnScheduler: Killing all running tasks in stage 44: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,622 INFO scheduler.DAGScheduler: Job 30 finished: collect at AnalysisRunner.scala:326, took 0.083478 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,657 INFO codegen.CodeGenerator: Code generated in 30.527782 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,813 INFO spark.SparkContext: Starting job: countByKey at ColumnProfiler.scala:592\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,814 INFO scheduler.DAGScheduler: Registering RDD 185 (countByKey at ColumnProfiler.scala:592) as input to shuffle 14\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,815 INFO scheduler.DAGScheduler: Got job 31 (countByKey at ColumnProfiler.scala:592) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,815 INFO scheduler.DAGScheduler: Final stage: ResultStage 46 (countByKey at ColumnProfiler.scala:592)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,815 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,815 INFO scheduler.DAGScheduler: Missing parents: List(ShuffleMapStage 45)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,817 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,870 INFO memory.MemoryStore: Block broadcast_37 stored as values in memory (estimated size 32.8 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,872 INFO memory.MemoryStore: Block broadcast_37_piece0 stored as bytes in memory (estimated size 14.9 KiB, free 1456.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,872 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on 10.0.79.62:35505 (size: 14.9 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,873 INFO spark.SparkContext: Created broadcast 37 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,873 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 45 (MapPartitionsRDD[185] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,873 INFO cluster.YarnScheduler: Adding task set 45.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,875 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 45.0 (TID 35) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,879 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on algo-1:41655 in memory (size: 19.8 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,881 INFO storage.BlockManagerInfo: Removed broadcast_29_piece0 on 10.0.79.62:35505 in memory (size: 19.8 KiB, free: 1458.2 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,896 INFO storage.BlockManagerInfo: Added broadcast_37_piece0 in memory on algo-1:41655 (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,935 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on algo-1:41655 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,937 INFO storage.BlockManagerInfo: Removed broadcast_30_piece0 on 10.0.79.62:35505 in memory (size: 14.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,976 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on 10.0.79.62:35505 in memory (size: 17.1 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:25,979 INFO storage.BlockManagerInfo: Removed broadcast_27_piece0 on algo-1:41655 in memory (size: 17.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,018 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on 10.0.79.62:35505 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,022 INFO storage.BlockManagerInfo: Removed broadcast_31_piece0 on algo-1:41655 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,033 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on 10.0.79.62:35505 in memory (size: 17.4 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,041 INFO storage.BlockManagerInfo: Removed broadcast_35_piece0 on algo-1:41655 in memory (size: 17.4 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,044 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on 10.0.79.62:35505 in memory (size: 16.9 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,048 INFO storage.BlockManagerInfo: Removed broadcast_34_piece0 on algo-1:41655 in memory (size: 16.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,051 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on 10.0.79.62:35505 in memory (size: 28.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,055 INFO storage.BlockManagerInfo: Removed broadcast_25_piece0 on algo-1:41655 in memory (size: 28.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,062 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on 10.0.79.62:35505 in memory (size: 3.0 KiB, free: 1458.3 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,066 INFO storage.BlockManagerInfo: Removed broadcast_24_piece0 on algo-1:41655 in memory (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,069 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on 10.0.79.62:35505 in memory (size: 11.5 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,071 INFO storage.BlockManagerInfo: Removed broadcast_36_piece0 on algo-1:41655 in memory (size: 11.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,074 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on 10.0.79.62:35505 in memory (size: 47.0 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,082 INFO storage.BlockManagerInfo: Removed broadcast_26_piece0 on algo-1:41655 in memory (size: 47.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,086 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on 10.0.79.62:35505 in memory (size: 41.1 KiB, free: 1458.4 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,088 INFO storage.BlockManagerInfo: Removed broadcast_33_piece0 on algo-1:41655 in memory (size: 41.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,091 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on 10.0.79.62:35505 in memory (size: 25.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,094 INFO storage.BlockManagerInfo: Removed broadcast_28_piece0 on algo-1:41655 in memory (size: 25.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,099 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on 10.0.79.62:35505 in memory (size: 25.7 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,101 INFO storage.BlockManagerInfo: Removed broadcast_32_piece0 on algo-1:41655 in memory (size: 25.7 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,104 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on 10.0.79.62:35505 in memory (size: 14.9 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,105 INFO storage.BlockManagerInfo: Removed broadcast_23_piece0 on algo-1:41655 in memory (size: 14.9 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,324 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 45.0 (TID 35) in 449 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,324 INFO cluster.YarnScheduler: Removed TaskSet 45.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,325 INFO scheduler.DAGScheduler: ShuffleMapStage 45 (countByKey at ColumnProfiler.scala:592) finished in 0.507 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,325 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,326 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,326 INFO scheduler.DAGScheduler: waiting: Set(ResultStage 46)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,326 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,326 INFO scheduler.DAGScheduler: Submitting ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,327 INFO memory.MemoryStore: Block broadcast_38 stored as values in memory (estimated size 5.1 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,329 INFO memory.MemoryStore: Block broadcast_38_piece0 stored as bytes in memory (estimated size 3.0 KiB, free 1457.7 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,329 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on 10.0.79.62:35505 (size: 3.0 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,329 INFO spark.SparkContext: Created broadcast 38 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,330 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 46 (ShuffledRDD[186] at countByKey at ColumnProfiler.scala:592) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,330 INFO cluster.YarnScheduler: Adding task set 46.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,331 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 46.0 (TID 36) (algo-1, executor 1, partition 0, NODE_LOCAL, 4282 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,340 INFO storage.BlockManagerInfo: Added broadcast_38_piece0 in memory on algo-1:41655 (size: 3.0 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,342 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 14 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,353 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 46.0 (TID 36) in 22 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,353 INFO cluster.YarnScheduler: Removed TaskSet 46.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,354 INFO scheduler.DAGScheduler: ResultStage 46 (countByKey at ColumnProfiler.scala:592) finished in 0.027 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,354 INFO scheduler.DAGScheduler: Job 31 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,354 INFO cluster.YarnScheduler: Killing all running tasks in stage 46: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,354 INFO scheduler.DAGScheduler: Job 31 finished: countByKey at ColumnProfiler.scala:592, took 0.540711 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,598 INFO FileUtil: Write to file constraints.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,625 INFO codegen.CodeGenerator: Code generated in 7.904513 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,632 INFO scheduler.DAGScheduler: Registering RDD 191 (count at StatsGenerator.scala:66) as input to shuffle 15\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,632 INFO scheduler.DAGScheduler: Got map stage job 32 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,632 INFO scheduler.DAGScheduler: Final stage: ShuffleMapStage 47 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,632 INFO scheduler.DAGScheduler: Parents of final stage: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,633 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,633 INFO scheduler.DAGScheduler: Submitting ShuffleMapStage 47 (MapPartitionsRDD[191] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,636 INFO memory.MemoryStore: Block broadcast_39 stored as values in memory (estimated size 24.8 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,637 INFO memory.MemoryStore: Block broadcast_39_piece0 stored as bytes in memory (estimated size 11.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,638 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on 10.0.79.62:35505 (size: 11.1 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,638 INFO spark.SparkContext: Created broadcast 39 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,638 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ShuffleMapStage 47 (MapPartitionsRDD[191] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,638 INFO cluster.YarnScheduler: Adding task set 47.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,640 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 47.0 (TID 37) (algo-1, executor 1, partition 0, PROCESS_LOCAL, 4953 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,648 INFO storage.BlockManagerInfo: Added broadcast_39_piece0 in memory on algo-1:41655 (size: 11.1 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,685 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 47.0 (TID 37) in 46 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,685 INFO cluster.YarnScheduler: Removed TaskSet 47.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,685 INFO scheduler.DAGScheduler: ShuffleMapStage 47 (count at StatsGenerator.scala:66) finished in 0.051 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,686 INFO scheduler.DAGScheduler: looking for newly runnable stages\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,686 INFO scheduler.DAGScheduler: running: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,686 INFO scheduler.DAGScheduler: waiting: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,686 INFO scheduler.DAGScheduler: failed: Set()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,702 INFO codegen.CodeGenerator: Code generated in 7.287501 ms\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,711 INFO spark.SparkContext: Starting job: count at StatsGenerator.scala:66\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,712 INFO scheduler.DAGScheduler: Got job 33 (count at StatsGenerator.scala:66) with 1 output partitions\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,712 INFO scheduler.DAGScheduler: Final stage: ResultStage 49 (count at StatsGenerator.scala:66)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,712 INFO scheduler.DAGScheduler: Parents of final stage: List(ShuffleMapStage 48)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,712 INFO scheduler.DAGScheduler: Missing parents: List()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,712 INFO scheduler.DAGScheduler: Submitting ResultStage 49 (MapPartitionsRDD[194] at count at StatsGenerator.scala:66), which has no missing parents\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,714 INFO memory.MemoryStore: Block broadcast_40 stored as values in memory (estimated size 11.1 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,715 INFO memory.MemoryStore: Block broadcast_40_piece0 stored as bytes in memory (estimated size 5.5 KiB, free 1457.6 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,716 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on 10.0.79.62:35505 (size: 5.5 KiB, free: 1458.5 MiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,716 INFO spark.SparkContext: Created broadcast 40 from broadcast at DAGScheduler.scala:1513\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,716 INFO scheduler.DAGScheduler: Submitting 1 missing tasks from ResultStage 49 (MapPartitionsRDD[194] at count at StatsGenerator.scala:66) (first 15 tasks are for partitions Vector(0))\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,717 INFO cluster.YarnScheduler: Adding task set 49.0 with 1 tasks resource profile 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,718 INFO scheduler.TaskSetManager: Starting task 0.0 in stage 49.0 (TID 38) (algo-1, executor 1, partition 0, NODE_LOCAL, 4464 bytes) taskResourceAssignments Map()\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,727 INFO storage.BlockManagerInfo: Added broadcast_40_piece0 in memory on algo-1:41655 (size: 5.5 KiB, free: 5.8 GiB)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,730 INFO spark.MapOutputTrackerMasterEndpoint: Asked to send map output locations for shuffle 15 to 10.0.79.62:51980\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,744 INFO scheduler.TaskSetManager: Finished task 0.0 in stage 49.0 (TID 38) in 26 ms on algo-1 (executor 1) (1/1)\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,745 INFO cluster.YarnScheduler: Removed TaskSet 49.0, whose tasks have all completed, from pool \u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,746 INFO scheduler.DAGScheduler: ResultStage 49 (count at StatsGenerator.scala:66) finished in 0.033 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,746 INFO scheduler.DAGScheduler: Job 33 is finished. Cancelling potential speculative or zombie tasks for this job\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,746 INFO cluster.YarnScheduler: Killing all running tasks in stage 49: Stage finished\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:26,747 INFO scheduler.DAGScheduler: Job 33 finished: count at StatsGenerator.scala:66, took 0.035752 s\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,289 INFO FileUtil: Write to file statistics.json at path /opt/ml/processing/output.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,304 INFO cluster.YarnClientSchedulerBackend: Interrupting monitor thread\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,364 INFO cluster.YarnClientSchedulerBackend: Shutting down all executors\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,365 INFO cluster.YarnSchedulerBackend$YarnDriverEndpoint: Asking each executor to shut down\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,370 INFO cluster.YarnClientSchedulerBackend: YARN client scheduler backend Stopped\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,385 INFO spark.MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,457 INFO memory.MemoryStore: MemoryStore cleared\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,460 INFO storage.BlockManager: BlockManager stopped\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,469 INFO storage.BlockManagerMaster: BlockManagerMaster stopped\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,473 INFO scheduler.OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,542 INFO spark.SparkContext: Successfully stopped SparkContext\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,543 INFO Main: Completed: Job completed successfully with no violations.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,543 INFO Main: Write to file /opt/ml/output/message.\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,581 INFO util.ShutdownHookManager: Shutdown hook called\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,581 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-d60aa5d2-6b1f-48db-bc0a-2c62cd58f254\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,600 INFO util.ShutdownHookManager: Deleting directory /tmp/spark-1ff2a661-b6b5-44f6-a39f-16b5fbec95cf\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,706 - DefaultDataAnalyzer - INFO - Completed spark-submit with return code : 0\u001b[0m\n",
      "\u001b[34m2025-06-09 08:49:27,707 - DefaultDataAnalyzer - INFO - Spark job completed.\u001b[0m\n",
      "\n",
      "Baseline generation complete\n"
     ]
    }
   ],
   "source": [
    "# Create model monitor object\n",
    "monitor = DefaultModelMonitor(\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    volume_size_in_gb=20,\n",
    "    max_runtime_in_seconds=3600,\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Define baseline input/output locations\n",
    "baseline_data_uri = f's3://{bucket}/{prefix}/cardio_engineered_clean.csv'\n",
    "baseline_results_uri = f's3://{bucket}/{prefix}/baseline-results'\n",
    "\n",
    "# Suggest baseline job\n",
    "baseline_job = monitor.suggest_baseline(\n",
    "    baseline_dataset=baseline_data_uri,\n",
    "    dataset_format={'csv': {'header': True}},\n",
    "    output_s3_uri=baseline_results_uri,\n",
    "    wait=True\n",
    ")\n",
    "\n",
    "print(\"Baseline generation complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cd3fc0e-fd1f-4e0e-923a-4e325d564425",
   "metadata": {},
   "source": [
    "Full model monitoring pipeline has been prepared by first creating the SKLearn model object, which wraps the trained logistic regression model and inference script so SageMaker can use it. Then I enable data capture, which collects real-time input data and predictions as the model serves traffic, creating a record of live inference data. After that, I deploy the model as an endpoint to make it accessible for inference while capturing data. Once deployed, I generate a baseline using the fully cleaned and preprocessed training dataset; this allows SageMaker to compute statistics and constraints that define the models expected data distributions. These baseline statistics become the reference point that future data will be compared against to detect drift or anomalies. This full setup ensures that once monitoring jobs are configured, SageMaker can automatically evaluate incoming data for shifts that may impact model accuracy or stability."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5636aeb4-cfe1-4999-96b1-97a027ad988e",
   "metadata": {},
   "source": [
    "* Calculates <b>statistics</b> (distribution, min, max, mean, std, percentiles, etc.)\n",
    "* Generates <b>constraints</b> (rules/thresholds learned from your dataset, e.g., feature X must be within certain boundaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c285f5f8-e12c-40cd-b2c8-55a9876ee418",
   "metadata": {},
   "source": [
    "### Check Constraint Files Exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "78f73e78-2dc6-465c-92d4-7f2a261f186d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline statistics and constraints found:\n",
      "cardio_data/baseline-results/constraints.json\n",
      "cardio_data/baseline-results/statistics.json\n"
     ]
    }
   ],
   "source": [
    "# Initialize S3 client\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Your bucket and baseline folder\n",
    "bucket = 'sagemaker-us-east-1-531690656306'\n",
    "baseline_prefix = 'cardio_data/baseline-results/'\n",
    "\n",
    "# List objects in the baseline folder\n",
    "response = s3.list_objects_v2(Bucket=bucket, Prefix=baseline_prefix)\n",
    "\n",
    "# Check and print files\n",
    "if 'Contents' in response:\n",
    "    print(\"Baseline statistics and constraints found:\")\n",
    "    for obj in response['Contents']:\n",
    "        print(obj['Key'])\n",
    "else:\n",
    "    print(\"No baseline files found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0392fe0e-bfe3-4536-8202-1627f6b1f591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./cardio_model_monitoring.ipynb to s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_model_monitoring.ipynb\n"
     ]
    }
   ],
   "source": [
    "# Save Notebook to S3\n",
    "!aws s3 cp cardio_model_monitoring.ipynb s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_model_monitoring.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "643765fb-250d-4260-9c23-7499010bef28",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## For Reference Only\n",
    "### <u>File Locations Summary for Model Monitoring Setup</u>\n",
    "\n",
    "#### Trained Model Artifact (used for deployment & endpoint)\n",
    "`s3://sagemaker-us-east-1-531690656306/model/logistic_model.tar.gz`\n",
    "\n",
    "#### Training Dataset (used for baseline generation)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_train.csv`\n",
    "\n",
    "#### Engineered Dataset (cleaned and engineered)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_engineered_clean.csv`\n",
    "\n",
    "#### Baseline Results Output Folder (generated by model monitor baseline job)\n",
    "`s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/`\n",
    "\n",
    "#### Inside baseline-results:\n",
    "- `statistics.json`  \n",
    "  `s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/statistics.json`\n",
    "\n",
    "- `constraints.json`  \n",
    "  `s3://sagemaker-us-east-1-531690656306/cardio_data/baseline-results/constraints.json`\n",
    "\n",
    "#### Data Capture Location (request/response payloads captured from endpoint)\n",
    "`s3://sagemaker-us-east-1-531690656306/data-capture/`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df142b91-3dc6-43af-98a7-61b3edcdec3b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sagemaker-env)",
   "language": "python",
   "name": "sagemaker-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
