{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e99f61-4b19-4b2b-a545-f16cc3cc084f",
   "metadata": {},
   "source": [
    "### Create inference and pkl for the training feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91ed8cfa-a7ba-4396-afa4-53d20dc99d6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in /opt/conda/lib/python3.12/site-packages (25.0.1)\n",
      "Collecting pip\n",
      "  Using cached pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
      "Using cached pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
      "Installing collected packages: pip\n",
      "  Attempting uninstall: pip\n",
      "    Found existing installation: pip 25.0.1\n",
      "    Uninstalling pip-25.0.1:\n",
      "      Successfully uninstalled pip-25.0.1\n",
      "Successfully installed pip-25.1.1\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d648df54-c40b-4d8e-8aa5-b07bdf5a7276",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas: 1.0.5\n",
      "scikit-learn: 0.23.2\n",
      "joblib: 0.14.1\n",
      "boto3: 1.17.106\n",
      "sagemaker: 2.34.0\n",
      "protobuf: 3.12.0\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sklearn\n",
    "import joblib\n",
    "import boto3\n",
    "import sagemaker\n",
    "import google.protobuf\n",
    "\n",
    "print(\"pandas:\", pd.__version__)\n",
    "print(\"scikit-learn:\", sklearn.__version__)\n",
    "print(\"joblib:\", joblib.__version__)\n",
    "print(\"boto3:\", boto3.__version__)\n",
    "print(\"sagemaker:\", sagemaker.__version__)\n",
    "print(\"protobuf:\", google.protobuf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9fd7d87d-3542-49ed-bdee-82cf6c107363",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import tarfile\n",
    "import joblib\n",
    "import boto3\n",
    "import os\n",
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import Session\n",
    "import sagemaker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ae0b1431-30d7-4ae0-8a67-cd632e370e24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        age  height_ft  weight_lbs  systolic_bp  diastolic_bp  cholesterol  \\\n",
      "0 -0.860483  -0.323580   -0.914107    -0.410657     -0.147087    -0.538657   \n",
      "1 -0.268438   1.211180    0.266123    -0.410657     -0.147087    -0.538657   \n",
      "2  0.915651  -1.052592   -0.983384    -0.410657     -0.147087    -0.538657   \n",
      "3 -0.712472  -0.822378   -1.191845    -0.410657     -0.147087    -0.538657   \n",
      "4  0.323606  -0.170104   -0.358631    -0.410657     -0.147087    -0.538657   \n",
      "\n",
      "       gluc     smoke     alco    active  ...  age_years  is_hypertensive  \\\n",
      "0 -0.390761 -0.312731 -0.23822  0.494625  ...  -0.860483        -0.607947   \n",
      "1 -0.390761 -0.312731 -0.23822 -2.021734  ...  -0.268438        -0.607947   \n",
      "2 -0.390761 -0.312731 -0.23822  0.494625  ...   0.915651        -0.607947   \n",
      "3 -0.390761 -0.312731 -0.23822  0.494625  ...  -0.712472        -0.607947   \n",
      "4 -0.390761 -0.312731 -0.23822  0.494625  ...   0.323606        -0.607947   \n",
      "\n",
      "   age_gluc_interaction  lifestyle_score  gender  bp_category  bmi_category  \\\n",
      "0             -0.543807        -0.596864     0.0          2.0           0.0   \n",
      "1             -0.422052         1.161966     1.0          2.0           2.0   \n",
      "2             -0.178543        -0.596864     0.0          2.0           0.0   \n",
      "3             -0.513368        -0.596864     0.0          2.0           0.0   \n",
      "4             -0.300298        -0.596864     0.0          2.0           2.0   \n",
      "\n",
      "   age_group  cholesterol_label  cardio  \n",
      "0        1.0                1.0       0  \n",
      "1        2.0                1.0       0  \n",
      "2        2.0                1.0       1  \n",
      "3        1.0                1.0       0  \n",
      "4        2.0                1.0       0  \n",
      "\n",
      "[5 rows x 24 columns]\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv(\"s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_split40.csv\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6cb0209-d485-4834-93df-2c442c27f579",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "logistic_model.tar.gz created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Generate_model_and_inference.py\n",
    "\n",
    "# Load full dataset for inference preparation\n",
    "df = pd.read_csv(\"s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_split40.csv\")  # Thai's Path\n",
    "# df = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\")  # Prema's Path\n",
    "\n",
    "# Drop label column for inference\n",
    "df.drop(columns=[\"cardio\"], inplace=True)\n",
    "\n",
    "# Save as no-label inference CSV (no header as expected by SageMaker)\n",
    "df.to_csv(\"cardio_prod_no_label.csv\", index=False, header=False)\n",
    "\n",
    "# Load full dataset again for training\n",
    "df_train = pd.read_csv(\"s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_split40.csv\")  # Thai's Path\n",
    "# df_train = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\")  # Prema's Path\n",
    "\n",
    "# Prepare features & labels\n",
    "X = df_train.drop(columns=[\"cardio\"])\n",
    "y = df_train[\"cardio\"]\n",
    "\n",
    "# Encode any object-type features if still present\n",
    "# for col in X.select_dtypes(include=\"object\").columns:\n",
    "    # le = LabelEncoder()\n",
    "    # X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Train logistic regression model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model with compatible pickle protocol\n",
    "joblib.dump(model, \"logistic_model.pkl\", protocol=4)\n",
    "\n",
    "# Create inference.py file\n",
    "inference_code = '''\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "# Define feature columns matching your final dataset (23 columns)\n",
    "FEATURE_COLUMNS = [\n",
    "    'age', 'gender', 'height_ft', 'weight_lbs', 'systolic_bp', 'diastolic_bp',\n",
    "    'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'bmi',\n",
    "    'age_group', 'cholesterol_label', 'pulse_pressure', 'chol_bmi_ratio',\n",
    "    'height_in', 'age_years', 'is_hypertensive', 'bp_category', 'bmi_category',\n",
    "    'age_gluc_interaction', 'lifestyle_score'\n",
    "]\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    return joblib.load(os.path.join(model_dir, \"logistic_model.pkl\"))\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    if content_type == \"text/csv\":\n",
    "        df = pd.read_csv(StringIO(input_data), header=None)\n",
    "        if df.shape[1] != len(FEATURE_COLUMNS):\n",
    "            raise ValueError(f\"Column mismatch: expected {len(FEATURE_COLUMNS)} columns, got {df.shape[1]}\")\n",
    "        df.columns = FEATURE_COLUMNS\n",
    "        return df\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported content type: {content_type}\")\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return '\\\\n'.join(str(x) for x in prediction)\n",
    "'''\n",
    "\n",
    "# Write inference.py\n",
    "with open(\"inference.py\", \"w\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "model_filename = \"logistic_model.pkl\"\n",
    "\n",
    "# Package model and inference code\n",
    "tar_filename = 'logistic_model.tar.gz'\n",
    "with tarfile.open(tar_filename, \"w:gz\") as tar:\n",
    "    tar.add(model_filename)\n",
    "    tar.add(\"inference.py\")\n",
    "\n",
    "print(\"logistic_model.tar.gz created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05bb6b0e-8ece-4db3-b901-2059b5a98120",
   "metadata": {},
   "source": [
    "### Save files directly into S3 Bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "667e90c5-3349-48ae-b696-46106ef190bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files uploaded successfully!\n"
     ]
    }
   ],
   "source": [
    "# Define your bucket and prefix\n",
    "bucket = 'sagemaker-us-east-1-531690656306' # Thai's Path\n",
    "prefix = 'model'\n",
    "\n",
    "# Your filenames (assuming you created these earlier)\n",
    "model_filename = 'logistic_model.pkl'\n",
    "tar_filename = 'logistic_model.tar.gz'\n",
    "inference_csv_file = 'cardio_prod_no_label.csv'\n",
    "\n",
    "# Upload all files to S3 (overwrite automatically)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "s3_client.upload_file(model_filename, bucket, f\"{prefix}/{model_filename}\")\n",
    "s3_client.upload_file(\"inference.py\", bucket, f\"{prefix}/inference.py\")\n",
    "s3_client.upload_file(tar_filename, bucket, f\"{prefix}/{tar_filename}\")\n",
    "s3_client.upload_file(inference_csv_file, bucket, f\"{prefix}/{inference_csv_file}\")\n",
    "\n",
    "print(\"All files uploaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4f2d8-9389-4f5b-83ae-25c4d69ce309",
   "metadata": {},
   "source": [
    "### Compare Features in Training vs Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d708e3a3-a252-42f5-8bd1-942300950d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features Count: 23\n",
      "Inference Features Count: 23\n",
      "\n",
      "Training Feature Types:\n",
      "age                     float64\n",
      "height_ft               float64\n",
      "weight_lbs              float64\n",
      "systolic_bp             float64\n",
      "diastolic_bp            float64\n",
      "cholesterol             float64\n",
      "gluc                    float64\n",
      "smoke                   float64\n",
      "alco                    float64\n",
      "active                  float64\n",
      "bmi                     float64\n",
      "pulse_pressure          float64\n",
      "chol_bmi_ratio          float64\n",
      "height_in               float64\n",
      "age_years               float64\n",
      "is_hypertensive         float64\n",
      "age_gluc_interaction    float64\n",
      "lifestyle_score         float64\n",
      "gender                  float64\n",
      "bp_category             float64\n",
      "bmi_category            float64\n",
      "age_group               float64\n",
      "cholesterol_label       float64\n",
      "dtype: object\n",
      "\n",
      "Inference Data Types:\n",
      "age                     float64\n",
      "height_ft               float64\n",
      "weight_lbs              float64\n",
      "systolic_bp             float64\n",
      "diastolic_bp            float64\n",
      "cholesterol             float64\n",
      "gluc                    float64\n",
      "smoke                   float64\n",
      "alco                    float64\n",
      "active                  float64\n",
      "bmi                     float64\n",
      "pulse_pressure          float64\n",
      "chol_bmi_ratio          float64\n",
      "height_in               float64\n",
      "age_years               float64\n",
      "is_hypertensive         float64\n",
      "age_gluc_interaction    float64\n",
      "lifestyle_score         float64\n",
      "gender                  float64\n",
      "bp_category             float64\n",
      "bmi_category            float64\n",
      "age_group               float64\n",
      "cholesterol_label       float64\n",
      "dtype: object\n",
      "\n",
      "All feature columns match in name and dtype.\n"
     ]
    }
   ],
   "source": [
    "# Load the training data (with label)\n",
    "# df_train = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\") # Prema's Path\n",
    "df_train = pd.read_csv(\"s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_split40.csv\") # Thai's path\n",
    "train_features = df_train.drop(columns=[\"cardio\"]).columns.tolist()\n",
    "\n",
    "# Load the inference data (no label)\n",
    "df_infer = pd.read_csv(\"cardio_prod_no_label.csv\", header=None)\n",
    "\n",
    "# Compare number of columns\n",
    "print(\"Training Features Count:\", len(train_features))\n",
    "print(\"Inference Features Count:\", df_infer.shape[1])\n",
    "\n",
    "# Set column names on inference data to match training features for manual inspection (Optional)\n",
    "df_infer.columns = train_features\n",
    "\n",
    "# Compare data types and column names\n",
    "print(\"\\nTraining Feature Types:\")\n",
    "print(df_train[train_features].dtypes)\n",
    "\n",
    "print(\"\\nInference Data Types:\")\n",
    "print(df_infer.dtypes)\n",
    "\n",
    "# Identify mismatched columns (by type or order)\n",
    "mismatch_columns = [\n",
    "    (col, df_train[col].dtype, df_infer[col].dtype)\n",
    "    for col in train_features\n",
    "    if df_train[col].dtype != df_infer[col].dtype\n",
    "]\n",
    "\n",
    "if mismatch_columns:\n",
    "    print(\"\\nMismatched Columns Found:\")\n",
    "    for col, train_type, infer_type in mismatch_columns:\n",
    "        print(f\"   - {col}: training type = {train_type}, inference type = {infer_type}\")\n",
    "else:\n",
    "    print(\"\\nAll feature columns match in name and dtype.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276e51-ddc5-4d49-8f30-81a2ca216ed8",
   "metadata": {},
   "source": [
    "### Creating inference.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0c9cab29-536e-46b7-9f45-10e750d00289",
   "metadata": {},
   "outputs": [],
   "source": [
    "with tarfile.open(\"logistic_model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"logistic_model.pkl\")\n",
    "    tar.add(\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353554e-c5a4-4892-9b3d-d6978679ed83",
   "metadata": {},
   "source": [
    "### Sagemaker Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0f4da26f-57a3-4e2f-8b61-95a0a2ecfda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34m2025-06-11 02:02:58,464 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,468 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,469 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3855 sha256=deb4ada6bd6eae99496cc9ab39b76403b9d4809d3900e09f0bc50f4d3d2e4edb\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-ku52uu1b/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Listening at: unix:/tmp/gunicorn.sock (30)\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:01 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m2025-06-11 02:03:07,662 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-06-11 02:03:07,662 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2025-06-11 02:03:09,007 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2025-06-11 02:03:09,007 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 33935 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 20771 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 33935 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 20771 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-06-11T02:03:08.410:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "SageMaker Batch Transform job started for data: s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_no_label.csv\n",
      "Output will be saved to: s3://sagemaker-us-east-1-531690656306/cardio_data/predictions/\n",
      "\u001b[34m2025-06-11 02:02:58,464 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,468 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,469 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,464 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,468 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,469 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3855 sha256=deb4ada6bd6eae99496cc9ab39b76403b9d4809d3900e09f0bc50f4d3d2e4edb\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-ku52uu1b/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,708 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2025-06-11 02:02:58,709 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\n",
      "  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3855 sha256=deb4ada6bd6eae99496cc9ab39b76403b9d4809d3900e09f0bc50f4d3d2e4edb\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-ku52uu1b/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Listening at: unix:/tmp/gunicorn.sock (30)\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [30] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:00 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m[2025-06-11 02:03:01 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m[2025-06-11 02:03:00 +0000] [30] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2025-06-11 02:03:00 +0000] [30] [INFO] Listening at: unix:/tmp/gunicorn.sock (30)\u001b[0m\n",
      "\u001b[35m[2025-06-11 02:03:00 +0000] [30] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-06-11 02:03:00 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35m[2025-06-11 02:03:01 +0000] [34] [INFO] Booting worker with pid: 34\u001b[0m\n",
      "\u001b[34m2025-06-11 02:03:07,662 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-06-11 02:03:07,662 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2025-06-11 02:03:09,007 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2025-06-11 02:03:09,007 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 33935 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 20771 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 33935 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [11/Jun/2025:02:03:09 +0000] \"POST /invocations HTTP/1.1\" 200 20771 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-06-11T02:03:08.410:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Get SageMaker session and role\n",
    "session = Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the model\n",
    "model = SKLearnModel(\n",
    "    # model_data=\"s3://sagemaker-us-east-1-381492296191/cardio_data/logistic_model.tar.gz\", # Prema's path\n",
    "    model_data=\"s3://sagemaker-us-east-1-531690656306/model/logistic_model.tar.gz\", # Thai's path\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Create a transformer object\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    # output_path=\"s3://sagemaker-us-east-1-381492296191/cardio_data/predictions/\", # Prema's Path\n",
    "    output_path=\"s3://sagemaker-us-east-1-531690656306/cardio_data/predictions/\", # Thai's Path\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "# Define input data location\n",
    "# production_data_s3_uri = \"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_no_label.csv\" # Prema's Path\n",
    "production_data_s3_uri = \"s3://sagemaker-us-east-1-531690656306/cardio_data/cardio_prod_no_label.csv\" # Thai's Path\n",
    "\n",
    "# Run batch transform\n",
    "transformer.transform(\n",
    "    data=production_data_s3_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "print(f\"SageMaker Batch Transform job started for data: {production_data_s3_uri}\")\n",
    "# print(f\"Output will be saved to: s3://sagemaker-us-east-1-381492296191/cardio_data/predictions/\") # Prema's Path\n",
    "print(f\"Output will be saved to: s3://sagemaker-us-east-1-531690656306/cardio_data/predictions/\") # Thai's Path\n",
    "\n",
    "# Wait for completion\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "848aa272-3521-4440-b6f2-4012cd67e394",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ./cardio_inference_transform_job_v2.ipynb to s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_inference_transform_job_v2.ipynb\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp cardio_inference_transform_job_v2.ipynb s3://sagemaker-us-east-1-531690656306/cardio_project/cardio_inference_transform_job_v2.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c101deab-8b37-4996-bd9c-5467aba7cb12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (sagemaker-env)",
   "language": "python",
   "name": "sagemaker-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
