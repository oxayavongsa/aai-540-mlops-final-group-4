{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0e99f61-4b19-4b2b-a545-f16cc3cc084f",
   "metadata": {},
   "source": [
    "### Create inference and pkl for the training feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b36bb06b-4767-4dce-9f8d-5876ed8599ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ logistic_model.tar.gz created successfully.\n"
     ]
    }
   ],
   "source": [
    "# Generate_model_and_inference.py\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pickle\n",
    "import tarfile\n",
    "\n",
    "# Step 1: Load full dataset for inference preparation\n",
    "df = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\")\n",
    "df.drop(columns=[\"cardio\"], inplace=True)\n",
    "\n",
    "# Save as no-label inference CSV (no header as expected by SageMaker)\n",
    "df.to_csv(\"cardio_prod_no_label.csv\", index=False, header=False)\n",
    "\n",
    "# Step 2: Load full dataset for training\n",
    "df_train = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\")\n",
    "X = df_train.drop(columns=[\"cardio\"])\n",
    "y = df_train[\"cardio\"]\n",
    "\n",
    "# Step 3: Encode any object-type features if still present (shouldn't be anymore)\n",
    "for col in X.select_dtypes(include=\"object\").columns:\n",
    "    le = LabelEncoder()\n",
    "    X[col] = le.fit_transform(X[col])\n",
    "\n",
    "# Step 4: Train the model\n",
    "model = LogisticRegression(max_iter=1000)\n",
    "model.fit(X, y)\n",
    "\n",
    "# Step 5: Save model\n",
    "with open(\"logistic_model.pkl\", \"wb\") as f:\n",
    "    pickle.dump(model, f)\n",
    "\n",
    "# Step 6: Create inference.py\n",
    "inference_code = '''\\\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "# Define feature columns (excluding cholesterol_label)\n",
    "FEATURE_COLUMNS = [\n",
    "    'bmi', 'pulse_pressure', 'chol_bmi_ratio', 'age_gluc_interaction', 'age_years',\n",
    "    'bp_category', 'bmi_category', 'age_group', 'age', 'gender',\n",
    "    'systolic_bp', 'diastolic_bp', 'cholesterol', 'gluc', 'smoke',\n",
    "    'alco', 'active', 'is_hypertensive', 'lifestyle_score'\n",
    "]\n",
    "\n",
    "def model_fn(model_dir):\n",
    "    with open(f\"{model_dir}/logistic_model.pkl\", \"rb\") as f:\n",
    "        return pickle.load(f)\n",
    "\n",
    "def input_fn(input_data, content_type):\n",
    "    df = pd.read_csv(StringIO(input_data), header=None)\n",
    "    df.columns = FEATURE_COLUMNS\n",
    "    return df\n",
    "\n",
    "def predict_fn(input_data, model):\n",
    "    return model.predict(input_data)\n",
    "\n",
    "def output_fn(prediction, content_type):\n",
    "    return '\\\\n'.join(str(x) for x in prediction)\n",
    "'''\n",
    "\n",
    "with open(\"inference.py\", \"w\") as f:\n",
    "    f.write(inference_code)\n",
    "\n",
    "# Step 7: Package model and inference code\n",
    "with tarfile.open(\"logistic_model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"logistic_model.pkl\")\n",
    "    tar.add(\"inference.py\")\n",
    "\n",
    "print(\"‚úÖ logistic_model.tar.gz created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42a95639-2ee2-471d-a51e-beb85ccfd2e9",
   "metadata": {},
   "source": [
    "import joblib\n",
    "joblib.dump(model, 'logistic_model.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cefb86e2-1721-4405-93ef-c99a3543456a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['logistic_model.pkl']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import joblib\n",
    "\n",
    "# After training your model:\n",
    "joblib.dump(model, \"logistic_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d4f2d8-9389-4f5b-83ae-25c4d69ce309",
   "metadata": {},
   "source": [
    "### Compare Features in Training vs Inference Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d708e3a3-a252-42f5-8bd1-942300950d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Training Features Count: 19\n",
      "‚úÖ Inference Features Count: 19\n",
      "\n",
      "üìå Training Feature Types:\n",
      "bmi                     float64\n",
      "pulse_pressure          float64\n",
      "chol_bmi_ratio          float64\n",
      "age_gluc_interaction    float64\n",
      "age_years               float64\n",
      "bp_category               int64\n",
      "bmi_category              int64\n",
      "age_group                 int64\n",
      "age                       int64\n",
      "gender                    int64\n",
      "systolic_bp               int64\n",
      "diastolic_bp              int64\n",
      "cholesterol               int64\n",
      "gluc                      int64\n",
      "smoke                     int64\n",
      "alco                      int64\n",
      "active                    int64\n",
      "is_hypertensive           int64\n",
      "lifestyle_score           int64\n",
      "dtype: object\n",
      "\n",
      "üìå Inference Data Types:\n",
      "bmi                     float64\n",
      "pulse_pressure          float64\n",
      "chol_bmi_ratio          float64\n",
      "age_gluc_interaction    float64\n",
      "age_years               float64\n",
      "bp_category               int64\n",
      "bmi_category              int64\n",
      "age_group                 int64\n",
      "age                       int64\n",
      "gender                    int64\n",
      "systolic_bp               int64\n",
      "diastolic_bp              int64\n",
      "cholesterol               int64\n",
      "gluc                      int64\n",
      "smoke                     int64\n",
      "alco                      int64\n",
      "active                    int64\n",
      "is_hypertensive           int64\n",
      "lifestyle_score           int64\n",
      "dtype: object\n",
      "\n",
      "‚úÖ All feature columns match in name and dtype.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the training data (with label)\n",
    "df_train = pd.read_csv(\"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_split40.csv\")\n",
    "train_features = df_train.drop(columns=[\"cardio\"]).columns.tolist()\n",
    "\n",
    "# Load the inference data (no label)\n",
    "df_infer = pd.read_csv(\"cardio_prod_no_label.csv\", header=None)\n",
    "\n",
    "# Compare number of columns\n",
    "print(\"‚úÖ Training Features Count:\", len(train_features))\n",
    "print(\"‚úÖ Inference Features Count:\", df_infer.shape[1])\n",
    "\n",
    "# Optional: Set column names on inference data to match training features for manual inspection\n",
    "df_infer.columns = train_features\n",
    "\n",
    "# Compare data types and column names\n",
    "print(\"\\nüìå Training Feature Types:\")\n",
    "print(df_train[train_features].dtypes)\n",
    "\n",
    "print(\"\\nüìå Inference Data Types:\")\n",
    "print(df_infer.dtypes)\n",
    "\n",
    "# Identify mismatched columns (by type or order)\n",
    "mismatch_columns = [\n",
    "    (col, df_train[col].dtype, df_infer[col].dtype)\n",
    "    for col in train_features\n",
    "    if df_train[col].dtype != df_infer[col].dtype\n",
    "]\n",
    "\n",
    "if mismatch_columns:\n",
    "    print(\"\\n‚ùå Mismatched Columns Found:\")\n",
    "    for col, train_type, infer_type in mismatch_columns:\n",
    "        print(f\"   - {col}: training type = {train_type}, inference type = {infer_type}\")\n",
    "else:\n",
    "    print(\"\\n‚úÖ All feature columns match in name and dtype.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55ccc471-dc09-44f2-bcc7-f227a4211952",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Features:\n",
      " ['bmi', 'pulse_pressure', 'chol_bmi_ratio', 'age_gluc_interaction', 'age_years', 'bp_category', 'bmi_category', 'age_group', 'age', 'gender', 'systolic_bp', 'diastolic_bp', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'is_hypertensive', 'lifestyle_score']\n",
      "Inference Columns:\n",
      " ['bmi', 'pulse_pressure', 'chol_bmi_ratio', 'age_gluc_interaction', 'age_years', 'bp_category', 'bmi_category', 'age_group', 'age', 'gender', 'systolic_bp', 'diastolic_bp', 'cholesterol', 'gluc', 'smoke', 'alco', 'active', 'is_hypertensive', 'lifestyle_score']\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Features:\\n\", train_features)\n",
    "print(\"Inference Columns:\\n\", df_infer.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c276e51-ddc5-4d49-8f30-81a2ca216ed8",
   "metadata": {},
   "source": [
    "### Creating inference.py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0c9cab29-536e-46b7-9f45-10e750d00289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tarfile\n",
    "\n",
    "with tarfile.open(\"logistic_model.tar.gz\", \"w:gz\") as tar:\n",
    "    tar.add(\"logistic_model.pkl\")\n",
    "    tar.add(\"inference.py\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3353554e-c5a4-4892-9b3d-d6978679ed83",
   "metadata": {},
   "source": [
    "### Sagemaker Transform Job"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f4da26f-57a3-4e2f-8b61-95a0a2ecfda2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sagemaker.config INFO - Not applying SDK defaults from location: /etc/xdg/sagemaker/config.yaml\n",
      "sagemaker.config INFO - Not applying SDK defaults from location: /home/sagemaker-user/.config/sagemaker/config.yaml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating transform job with name: sagemaker-scikit-learn-2025-05-31-16-01-22-230\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "............................\u001b[34m2025-05-31 16:05:58,390 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,394 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,395 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3575 sha256=fca63f9cfd1ab20d3c60305c6f0e56d8a97e2e312f3976af147e03b87c65959f\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-4vyaqwd5/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Listening at: unix:/tmp/gunicorn.sock (29)\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:01 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:01 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m2025-05-31 16:06:07,190 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:07 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2025-05-31 16:06:07,881 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"POST /invocations HTTP/1.1\" 200 54707 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-05-31T16:06:08.376:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n",
      "\n",
      "SageMaker Batch Transform job started for data: s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_no_label.csv\n",
      "Output will be saved to: s3://sagemaker-us-east-1-381492296191/cardio_data/predictions/\n",
      "\u001b[34m2025-05-31 16:05:58,390 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,394 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,395 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[34mworker_processes auto;\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,390 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,394 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,395 INFO - sagemaker-containers - nginx config: \u001b[0m\n",
      "\u001b[35mworker_processes auto;\u001b[0m\n",
      "\u001b[34mdaemon off;\u001b[0m\n",
      "\u001b[34mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[34merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[34mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[34mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[34m}\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[34mGenerating setup.py\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[34m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[34m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[34mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[35mdaemon off;\u001b[0m\n",
      "\u001b[35mpid /tmp/nginx.pid;\u001b[0m\n",
      "\u001b[35merror_log  /dev/stderr;\u001b[0m\n",
      "\u001b[35mworker_rlimit_nofile 4096;\u001b[0m\n",
      "\u001b[35mevents {\n",
      "  worker_connections 2048;\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35mhttp {\n",
      "  include /etc/nginx/mime.types;\n",
      "  default_type application/octet-stream;\n",
      "  access_log /dev/stdout combined;\n",
      "  upstream gunicorn {\n",
      "    server unix:/tmp/gunicorn.sock;\n",
      "  }\n",
      "  server {\n",
      "    listen 8080 deferred;\n",
      "    client_max_body_size 0;\n",
      "    keepalive_timeout 3;\n",
      "    location ~ ^/(ping|invocations|execution-parameters) {\n",
      "      proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;\n",
      "      proxy_set_header Host $http_host;\n",
      "      proxy_redirect off;\n",
      "      proxy_read_timeout 60s;\n",
      "      proxy_pass http://gunicorn;\n",
      "    }\n",
      "    location / {\n",
      "      return 404 \"{}\";\n",
      "    }\n",
      "  }\u001b[0m\n",
      "\u001b[35m}\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Module inference does not provide a setup.py. \u001b[0m\n",
      "\u001b[35mGenerating setup.py\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating setup.cfg\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Generating MANIFEST.in\u001b[0m\n",
      "\u001b[35m2025-05-31 16:05:58,651 INFO - sagemaker-containers - Installing module with the following command:\u001b[0m\n",
      "\u001b[35m/miniconda3/bin/python3 -m pip install . \u001b[0m\n",
      "\u001b[35mProcessing /opt/ml/code\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\u001b[0m\n",
      "\u001b[34mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[35mBuilding wheels for collected packages: inference\n",
      "  Building wheel for inference (setup.py): started\u001b[0m\n",
      "\u001b[34m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3575 sha256=fca63f9cfd1ab20d3c60305c6f0e56d8a97e2e312f3976af147e03b87c65959f\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-4vyaqwd5/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[34mSuccessfully built inference\u001b[0m\n",
      "\u001b[34mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[34mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[34mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[34m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[34m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[35m  Building wheel for inference (setup.py): finished with status 'done'\n",
      "  Created wheel for inference: filename=inference-1.0.0-py2.py3-none-any.whl size=3575 sha256=fca63f9cfd1ab20d3c60305c6f0e56d8a97e2e312f3976af147e03b87c65959f\n",
      "  Stored in directory: /home/model-server/tmp/pip-ephem-wheel-cache-4vyaqwd5/wheels/3e/0f/51/2f1df833dd0412c1bc2f5ee56baac195b5be563353d111dca6\u001b[0m\n",
      "\u001b[35mSuccessfully built inference\u001b[0m\n",
      "\u001b[35mInstalling collected packages: inference\u001b[0m\n",
      "\u001b[35mSuccessfully installed inference-1.0.0\u001b[0m\n",
      "\u001b[35mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "\u001b[35m[notice] A new release of pip is available: 23.0 -> 24.0\u001b[0m\n",
      "\u001b[35m[notice] To update, run: pip install --upgrade pip\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Listening at: unix:/tmp/gunicorn.sock (29)\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:00 +0000] [29] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:01 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[34m[2025-05-31 16:06:01 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[35m[2025-05-31 16:06:00 +0000] [29] [INFO] Starting gunicorn 20.0.4\u001b[0m\n",
      "\u001b[35m[2025-05-31 16:06:00 +0000] [29] [INFO] Listening at: unix:/tmp/gunicorn.sock (29)\u001b[0m\n",
      "\u001b[35m[2025-05-31 16:06:00 +0000] [29] [INFO] Using worker: gevent\u001b[0m\n",
      "\u001b[35m[2025-05-31 16:06:01 +0000] [32] [INFO] Booting worker with pid: 32\u001b[0m\n",
      "\u001b[35m[2025-05-31 16:06:01 +0000] [33] [INFO] Booting worker with pid: 33\u001b[0m\n",
      "\u001b[34m2025-05-31 16:06:07,190 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m2025-05-31 16:06:07,190 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:07 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m2025-05-31 16:06:07,881 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[34m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/May/2025:16:06:07 +0000] \"GET /ping HTTP/1.1\" 200 0 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m2025-05-31 16:06:07,881 INFO - sagemaker-containers - No GPUs detected (normal if no gpus installed)\u001b[0m\n",
      "\u001b[35m/miniconda3/lib/python3.7/site-packages/sklearn/base.py:334: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.5.2 when using version 0.23.2. This might lead to breaking code or invalid results. Use at your own risk.\n",
      "  UserWarning)\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"GET /execution-parameters HTTP/1.1\" 404 232 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[34m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"POST /invocations HTTP/1.1\" 200 54707 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[35m169.254.255.130 - - [31/May/2025:16:06:08 +0000] \"POST /invocations HTTP/1.1\" 200 54707 \"-\" \"Go-http-client/1.1\"\u001b[0m\n",
      "\u001b[32m2025-05-31T16:06:08.376:[sagemaker logs]: MaxConcurrentTransforms=1, MaxPayloadInMB=6, BatchStrategy=MULTI_RECORD\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.sklearn.model import SKLearnModel\n",
    "from sagemaker import Session\n",
    "import sagemaker\n",
    "\n",
    "# Get SageMaker session and role\n",
    "session = Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "\n",
    "# Define the model\n",
    "model = SKLearnModel(\n",
    "    model_data=\"s3://sagemaker-us-east-1-381492296191/cardio_data/logistic_model.tar.gz\",\n",
    "    role=role,\n",
    "    entry_point=\"inference.py\",\n",
    "    framework_version=\"0.23-1\",\n",
    "    sagemaker_session=session\n",
    ")\n",
    "\n",
    "# Create a transformer object\n",
    "transformer = model.transformer(\n",
    "    instance_count=1,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    output_path=\"s3://sagemaker-us-east-1-381492296191/cardio_data/predictions/\",\n",
    "    accept=\"text/csv\"\n",
    ")\n",
    "\n",
    "# Define input data location\n",
    "production_data_s3_uri = \"s3://sagemaker-us-east-1-381492296191/cardio_data/cardio_prod_no_label.csv\"\n",
    "\n",
    "# Run batch transform\n",
    "transformer.transform(\n",
    "    data=production_data_s3_uri,\n",
    "    content_type=\"text/csv\",\n",
    "    split_type=\"Line\"\n",
    ")\n",
    "\n",
    "print(f\"SageMaker Batch Transform job started for data: {production_data_s3_uri}\")\n",
    "print(f\"Output will be saved to: s3://sagemaker-us-east-1-381492296191/cardio_data/predictions/\")\n",
    "\n",
    "# Wait for completion\n",
    "transformer.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aafe01f-4ef6-4c7a-8616-c302bb951db8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
